{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对整个集合按时间窗口进行划分，计算precision recall f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "时间窗\n",
    "train_start_date = '2018-01-05'\n",
    "train_end_date = '2018-01-30'\n",
    "\n",
    "test_start_date = '2018-01-10'\n",
    "test_end_date = '2018-02-07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Dropout, SpatialDropout1D, Activation, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import ReLU, PReLU, LeakyReLU, ELU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movies_dataset():\n",
    "    movie_data = pd.read_csv(data_dir + './dataset1/movie.csv')\n",
    "    movie_data = movie_data.rename(columns = {'评分': \"豆瓣网评分\"})\n",
    "    return movie_data\n",
    "\n",
    "def load_user_and_ratings() :\n",
    "    user_data = pd.read_csv(data_dir + './dataset1/user.csv')\n",
    "    user_data['评论时间'] = pd.to_datetime(user_data['评论时间'])\n",
    "    return user_data\n",
    "\n",
    "def call_data_process(dfuserin,dfmoviein):\n",
    "    ur1 = dfuserin.groupby(['用户ID']).评分.agg( {'user_rating_avg':np.mean ,\n",
    "                                           'user_rating_max':np.max ,\n",
    "                                           'user_rating_min':np.min ,\n",
    "                                           'user_rating_median':np.median\n",
    "                                          }).reset_index()\n",
    "\n",
    "    dfuserin['用户评论次数_观看电影个数'] = dfuserin.groupby(['用户ID'])['评分'].transform('count')\n",
    "    u_data = pd.merge(ur1,dfuserin,on='用户ID',how = 'inner')\n",
    "    user_and_movies_df = pd.merge(u_data,dfmoviein,on=['电影名','类型'],how='inner')\n",
    "    return user_and_movies_df\n",
    "\n",
    "def get_all_real_datas_change():\n",
    "    movies_df = load_movies_dataset()\n",
    "    userdata = load_user_and_ratings()\n",
    "    del userdata['用户名']\n",
    "    all_df = call_data_process(userdata,movies_df).copy(deep=True)\n",
    "    all_df = all_df.dropna(how='any', axis=0)\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_action(start_date, end_date):\n",
    "    \"\"\"\n",
    "    产生指定时间区间的行为数据\n",
    "    \"\"\"\n",
    "    if os.path.exists(action_cate8_path):\n",
    "        actions = pickle.load(open(action_cate8_path, 'rb'))\n",
    "    else:\n",
    "        actions = get_all_real_datas_change()\n",
    "    actions = actions[(actions.评论时间 >= start_date) & (actions.评论时间 < end_date)]\n",
    "    del actions['主演']\n",
    "    return actions\n",
    "\n",
    "def gen_labels( start_date,  end_date):\n",
    "    \"\"\"\n",
    "    产生交互日区间内的购买情况\n",
    "    \"\"\"\n",
    "    dump_path = my_dir + '/cache/labels_%s_%s.pkl' % ( start_date,  end_date)\n",
    "    if os.path.exists(dump_path):\n",
    "        labels = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        actions = gen_action( start_date,  end_date)\n",
    "        actions['喜欢'] = actions['评分'].apply(lambda x : 1 if x>=6 else 0)\n",
    "        labels = actions['喜欢'].copy()\n",
    "        del actions['喜欢']\n",
    "        pickle.dump(labels, open(dump_path, 'wb'))\n",
    "    return labels\n",
    "\n",
    "\n",
    "def gen_truth(act_start_date, act_end_date):\n",
    "    \"\"\"\n",
    "    产生交互日区间内的实际购买情况\n",
    "    \"\"\"\n",
    "    dump_path = my_dir + '/cache/truth_%s_%s.pkl' % (act_start_date, act_end_date)\n",
    "    if os.path.exists(dump_path):\n",
    "        actions = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        actions = gen_action(act_start_date, act_end_date)\n",
    "        actions['喜欢'] = actions['评分'].apply(lambda x: 1 if x >= 6 else 0)\n",
    "        pickle.dump(actions, open(dump_path, 'wb'))\n",
    "    return actions\n",
    "\n",
    "def make_train_set(train_start_date, train_end_date ):\n",
    "    \"\"\"\n",
    "    构造训练集\n",
    "    \"\"\"\n",
    "    dump_path = my_dir + '/cache/train_set_%s_%s.pkl' % (train_start_date, train_end_date)\n",
    "    print('make_train_set dump_path', dump_path)\n",
    "    if os.path.exists(dump_path):\n",
    "        train_set = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        train_set = gen_action(train_start_date, train_end_date)\n",
    "        label = gen_labels(train_start_date, train_end_date)\n",
    "    return train_set ,label\n",
    "\n",
    "def make_test_set(test_start_date, test_end_date):\n",
    "    \"\"\"\n",
    "    构造测试集\n",
    "    \"\"\"\n",
    "    dump_path =  my_dir +  '/cache/test_set_%s_%s.pkl' % (test_start_date, test_end_date)\n",
    "    if os.path.exists(dump_path):\n",
    "        test_set = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        test_set = gen_action(test_start_date, test_end_date)\n",
    "        pickle.dump(test_set, open(dump_path, 'wb'))\n",
    "\n",
    "    index = test_set[['用户ID']].copy()\n",
    "    return index, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(all_real_data):\n",
    "    # 缺失值删除\n",
    "    all_real_data.dropna(how='any', axis=0)\n",
    "    # 给数据\n",
    "    # 标签2值化\n",
    "    all_data = all_real_data.copy(deep=True)\n",
    "    all_data[LABEL_COLUMN] = all_data['评分'].apply(lambda x: 1 if x>=6 else 0 )\n",
    "    all_data.pop('评分')\n",
    "    # 标签y\n",
    "    y = all_data[LABEL_COLUMN].values\n",
    "    # 数据X\n",
    "    all_data.pop(LABEL_COLUMN)\n",
    "    # 类别型的label encoding\n",
    "    for c in CATEGORICAL_COLUMNS:\n",
    "        le = LabelEncoder()\n",
    "        all_data[c] = le.fit_transform(all_data[c])\n",
    "    #时间序列数据——————》打乱数据？？？\n",
    "    #随机打乱数据\n",
    "    #all_data = shuffle(all_data)\n",
    "    # 分别取出train和test的特征和标签\n",
    "    # 打乱后取95%作为训练集 5%作为测试集\n",
    "    train_size = int(all_data.shape[0])#*0.95\n",
    "    x_train = all_data.iloc[:train_size]\n",
    "    #print('x_train.shape',x_train.shape)\n",
    "    y_train = y[:train_size]\n",
    "    #x_test = all_data.iloc[train_size:]\n",
    "    #y_test = y[train_size:]\n",
    "    # 类别型的列\n",
    "    x_train_categ = np.array(x_train[CATEGORICAL_COLUMNS])\n",
    "\n",
    "    #x_test_categ = np.array(x_test[CATEGORICAL_COLUMNS])\n",
    "    # 连续值的列\n",
    "    x_train_conti = np.array(x_train[CONTINUOUS_COLUMNS], dtype='float64')\n",
    "    #x_test_conti = np.array(x_test[CONTINUOUS_COLUMNS], dtype='float64')\n",
    "    # 对连续值的列做幅度缩放\n",
    "    scaler = StandardScaler()\n",
    "    x_train_conti = scaler.fit_transform(x_train_conti)\n",
    "    #x_test_conti = scaler.transform(x_test_conti)\n",
    "    #return [x_train, y_train, x_test, y_test, x_train_categ, x_test_categ, x_train_conti, x_test_conti, all_data]\n",
    "    return [x_train, y_train,   x_train_categ,  x_train_conti,  all_data]\n",
    "\n",
    "class Wide_and_Deep:\n",
    "    def __init__(self, data,mode='wide and deep'):\n",
    "        self.data = data\n",
    "        self.mode = mode\n",
    "        x_train, y_train,  x_train_categ,  x_train_conti,  all_data \\\n",
    "            = preprocessing(self.data)\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        #self.x_test = x_test\n",
    "        #self.y_test = y_test\n",
    "        self.x_train_categ = x_train_categ\n",
    "        #self.x_test_categ = x_test_categ\n",
    "        self.x_train_conti = x_train_conti\n",
    "        #self.x_test_conti = x_test_conti\n",
    "        self.all_data = all_data\n",
    "        self.poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "        self.x_train_categ_poly = self.poly.fit_transform(x_train_categ)\n",
    "        #self.x_test_categ_poly = self.poly.transform(x_test_categ)\n",
    "        self.categ_inputs = None\n",
    "        self.conti_input = None\n",
    "        self.deep_component_outlayer = None\n",
    "        self.logistic_input = None\n",
    "        self.model = None\n",
    "\n",
    "    # input_length: 输入序列的长度\n",
    "    # dim -->  输入的不同的词的个数\n",
    "    # embed_dim -->  输出维度\n",
    "    # embed_i shape\n",
    "    #  (None, 1, 2)\n",
    "    # flatten_i shape\n",
    "    #  (None, 2)\n",
    "    def deep_component(self):\n",
    "        # deep部分的组件\n",
    "        categ_inputs = []\n",
    "        categ_embeds = []\n",
    "        # 对类别型的列做embedding\n",
    "        count = 0\n",
    "        for i in range(len(CATEGORICAL_COLUMNS)):\n",
    "            input_i = Input(shape=(1,), dtype='int32')\n",
    "            dim = len(np.unique(self.all_data[CATEGORICAL_COLUMNS[i]]))\n",
    "            embed_dim = int(np.ceil(dim ** 0.25))\n",
    "            embed_i = Embedding(dim, embed_dim, input_length=1)(input_i)\n",
    "            flatten_i = Flatten()(embed_i)\n",
    "            categ_inputs.append(input_i)\n",
    "            categ_embeds.append(flatten_i)\n",
    "        # 连续值的列\n",
    "        conti_input = Input(shape=(len(CONTINUOUS_COLUMNS),))\n",
    "        conti_dense = Dense(256, use_bias=False)(conti_input)\n",
    "        # 拼接类别型的embedding和连续值特征\n",
    "        concat_embeds = concatenate([conti_dense] + categ_embeds)\n",
    "        # 激活层与BN层\n",
    "        concat_embeds = Activation('relu')(concat_embeds)\n",
    "        bn_concat = BatchNormalization()(concat_embeds)\n",
    "        # 全连接+激活层+BN层\n",
    "        fc1 = Dense(512, use_bias=False)(bn_concat)\n",
    "        ac1 = ReLU()(fc1)\n",
    "        bn1 = BatchNormalization()(ac1)\n",
    "        fc2 = Dense(256, use_bias=False)(bn1)\n",
    "        ac2 = ReLU()(fc2)\n",
    "        bn2 = BatchNormalization()(ac2)\n",
    "        fc3 = Dense(128)(bn2)\n",
    "        ac3 = ReLU()(fc3)\n",
    "\n",
    "        self.categ_inputs = categ_inputs\n",
    "        self.conti_input = conti_input\n",
    "        self.deep_component_outlayer = ac3\n",
    "\n",
    "    # self.x_train_categ_poly.shape[1] --> 37\n",
    "    def wide_component(self):\n",
    "        # wide部分的组件\n",
    "        dim = self.x_train_categ_poly.shape[1]\n",
    "        self.logistic_input = Input(shape=(dim,))\n",
    "\n",
    "    # X           *   W      = Y\n",
    "    # (None, 165) * (165,1)  = (None,1)\n",
    "    def create_model(self):\n",
    "        # wide+deep\n",
    "        self.deep_component()\n",
    "        self.wide_component()\n",
    "        if self.mode == 'wide and deep':\n",
    "            out_layer = concatenate([self.deep_component_outlayer, self.logistic_input])\n",
    "            inputs = [self.conti_input] + self.categ_inputs + [self.logistic_input]\n",
    "        elif self.mode == 'deep':\n",
    "            out_layer = self.deep_component_outlayer\n",
    "            inputs = [self.conti_input] + self.categ_inputs\n",
    "        else:\n",
    "            print('wrong mode')\n",
    "            return\n",
    "\n",
    "        output = Dense(1, activation='sigmoid')(out_layer)\n",
    "        self.model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # 训练\n",
    "    # x: 训练数据的 Numpy 数组\n",
    "    # y: 目标（标签）数据的 Numpy 数组\n",
    "    # self.model.fit(x=None, y=None,epochs=epochs, batch_size=batch_size)\n",
    "    def train_model(self, epochs=2, optimizer='adam', batch_size=128):\n",
    "        # 不同结构的训练\n",
    "\n",
    "        # 没有model的情况\n",
    "        if not self.model:\n",
    "            print('You have to create model first')\n",
    "            return\n",
    "\n",
    "        # 使用wide&deep的情况\n",
    "        if self.mode == 'wide and deep':\n",
    "            print('type wide and deep is \\n', type(self.x_train_categ))\n",
    "            input_data = [self.x_train_conti] + \\\n",
    "                         [self.x_train_categ[:, i] for i in range(self.x_train_categ.shape[1])] + \\\n",
    "                         [self.x_train_categ_poly]\n",
    "        # 只使用deep的情况\n",
    "        elif self.mode == 'deep':\n",
    "            input_data = [self.x_train_conti] + \\\n",
    "                         [self.x_train_categ[:, i] for i in range(self.x_train_categ.shape[1])]\n",
    "        else:\n",
    "            print('wrong mode')\n",
    "            return\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', ])\n",
    "        self.model.fit(input_data, self.y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # 评估\n",
    "    def evaluate_model(self):\n",
    "        # if not self.model:\n",
    "        #     print('You have to create model first')\n",
    "        #     return\n",
    "        #\n",
    "        # if self.mode == 'wide and deep':\n",
    "        #     input_data = [self.x_test_conti] + \\\n",
    "        #                  [self.x_test_categ[:, i] for i in range(self.x_test_categ.shape[1])] + \\\n",
    "        #                  [self.x_test_categ_poly]\n",
    "        #     print('input_data len', len(input_data))\n",
    "        # elif self.mode == 'deep':\n",
    "        #     input_data = [self.x_test_conti] + \\\n",
    "        #                  [self.x_test_categ[:, i] for i in range(self.x_test_categ.shape[1])]\n",
    "        # else:\n",
    "        #     print('wrong mode')\n",
    "        #     return\n",
    "        #\n",
    "        # loss, acc = self.model.evaluate(input_data, self.y_test)\n",
    "        # print(f'test_loss: {loss} - test_acc: {acc}')\n",
    "        pass\n",
    "\n",
    "    def save_model(self, filename='wide_and_deep.h5'):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    # 预测\n",
    "    def predict_model(self):\n",
    "        if not self.model:\n",
    "            print('You have to create model first')\n",
    "            return\n",
    "\n",
    "        if self.mode == 'wide and deep':\n",
    "            input_data = [self.x_all_conti] + \\\n",
    "                         [self.x_all_categ[:, i] for i in range(self.x_all_categ.shape[1])] + \\\n",
    "                         [self.x_all_categ_poly]\n",
    "        elif self.mode == 'deep':\n",
    "            input_data = [self.x_all_conti] + \\\n",
    "                         [self.x_all_categ[:, i] for i in range(self.x_all_categ.shape[1])]\n",
    "        else:\n",
    "            print('wrong mode')\n",
    "            return\n",
    "\n",
    "def preprocessing_rec(dfin):\n",
    "    # 缺失值删除\n",
    "    dfin.dropna(how='any', axis=0)\n",
    "    # 标签2值化\n",
    "    all_data = dfin.copy(deep=True)\n",
    "    reverse_all_data = pd.DataFrame(columns = all_data.columns.to_list())\n",
    "    # 类别型的label encoding\n",
    "    for c in CATEGORICAL_COLUMNS:\n",
    "        le = LabelEncoder()\n",
    "        all_data[c] = le.fit_transform(all_data[c])\n",
    "        reverse_all_data[c] = le.inverse_transform(all_data[c])\n",
    "    # 类别型的列\n",
    "    x_all_categ = np.array(all_data[CATEGORICAL_COLUMNS])\n",
    "    # 连续值的列\n",
    "    x_all_conti = np.array(all_data[CONTINUOUS_COLUMNS], dtype='float64')\n",
    "\n",
    "    # 对连续值的列做幅度缩放\n",
    "    scaler = StandardScaler()\n",
    "    x_all_conti = scaler.fit_transform(x_all_conti)\n",
    "    x_all_poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "    x_all_categ_poly = x_all_poly.fit_transform(x_all_categ)\n",
    "    return [ x_all_categ,  x_all_conti,x_all_categ_poly, all_data,reverse_all_data]\n",
    "\n",
    "def get_predictions(df_in):\n",
    "    dump_path = my_dir + '/cache/predictions.pkl'\n",
    "    if os.path.exists(dump_path):\n",
    "        all_data = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        x_all_categ,  x_all_conti,x_all_categ_poly,all_data,reverse_all_data = preprocessing_rec(df_in)\n",
    "        input_data = [x_all_conti] +\\\n",
    "            [ x_all_categ[:, i] for i in range( x_all_categ.shape[1])] +\\\n",
    "            [ x_all_categ_poly]\n",
    "        # firstly recover datas here\n",
    "        #恢复数据的\"类型\", \"特色\", \"导演\",\"电影名\"列\n",
    "        all_data[[\"类型\", \"特色\",\"地区\", \"导演\",\"电影名\"]] = reverse_all_data[[\"类型\", \"特色\",\"地区\", \"导演\",\"电影名\"]].copy(deep=True)\n",
    "        wide_deep_net = load_model('wide_and_deep.h5')\n",
    "        predictions = wide_deep_net.predict(input_data)\n",
    "        pda = pd.DataFrame(predictions, columns=['predictions'])\n",
    "        all_data['喜欢'] = pda['predictions']\n",
    "        all_data = all_data.dropna(how='any', axis=0)\n",
    "        pickle.dump(all_data, open(dump_path, 'wb'))\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_test():\n",
    "    train_start_date = '2018-01-05'\n",
    "    train_end_date = '2018-01-30'\n",
    "\n",
    "    test_start_date = '2018-01-10'\n",
    "    test_end_date = '2018-02-07'\n",
    "\n",
    "    train_X, train_Y = make_train_set(train_start_date, train_end_date)\n",
    "    assert train_X.shape[0] == train_Y.shape[0]\n",
    "    wide_deep_net = Wide_and_Deep(train_X)\n",
    "    wide_deep_net.create_model()\n",
    "    wide_deep_net.train_model()\n",
    "    wide_deep_net.save_model()\n",
    "    print('after training...')\n",
    "    test_index, test_X = make_test_set(test_start_date, test_end_date)\n",
    "    assert train_X.shape[1] == test_X.shape[1]\n",
    "    #预测用户'2018-01-10'~'2018-02-07'会看的电影\n",
    "    predict_data = get_predictions(test_X)\n",
    "    #实际上用户'2018-01-10'~'2018-02-07'用户会看的电影\n",
    "    truth = gen_truth(test_start_date, test_end_date)\n",
    "    return truth,predict_data\n",
    "\n",
    "def PrecisonRecall():\n",
    "    print('------calculate recall&precision---------')\n",
    "    dump_path_true = my_dir + '/cache/true.pkl'\n",
    "    dump_path_predict = my_dir + '/cache/predict.pkl'\n",
    "    if os.path.exists(dump_path_true) and os.path.exists(dump_path_predict):\n",
    "        true = pickle.load(open(dump_path_true, 'rb'))\n",
    "        predict = pickle.load(open(dump_path_predict, 'rb'))\n",
    "    else:\n",
    "        true, predict = offline_test()\n",
    "        pickle.dump(true, open(dump_path_true, 'wb'))\n",
    "        pickle.dump(predict, open(dump_path_predict, 'wb'))\n",
    "\n",
    "    true = true[['用户ID','电影名']]\n",
    "    true[\"用户真实喜欢的电影名\"] = true['电影名']\n",
    "    t1 = true.groupby('用户ID').电影名.size().reset_index()\n",
    "    t1 = t1.rename(columns={'电影名': \"用户实际观看电影个数\"})\n",
    "    true = pd.merge(true,t1,on='用户ID')\n",
    "    predict = predict[['用户ID', '电影名']]\n",
    "    predict[\"预测用户喜欢的电影名\"] = predict['电影名']\n",
    "    t2 = predict.groupby('用户ID').电影名.size().reset_index()\n",
    "    t2 = t2.rename(columns={'电影名': \"预测用户观看电影个数\"})\n",
    "    predict = pd.merge(predict, t2, on='用户ID')\n",
    "    del true['电影名']\n",
    "    del predict['电影名']\n",
    "    true.to_csv('./true.csv', index=False, index_label=False)\n",
    "    predict = predict.dropna(how='any', axis=0)\n",
    "    predict.to_csv('./predict.csv', index=False, index_label=False)\n",
    "    hit = 0\n",
    "    predict_watched = 0\n",
    "    real_watched = 0\n",
    "    user_real_list = true.用户ID.unique()\n",
    "    user_predict_list = predict.用户ID.unique()\n",
    "    for i in user_predict_list:\n",
    "        predict_movie_list = predict[predict.用户ID==i]['预测用户喜欢的电影名'].values\n",
    "        for j in  user_real_list:\n",
    "            #首先需要用户id相等\n",
    "            if i == j:\n",
    "                real_movie_list = true[true.用户ID==j]['用户真实喜欢的电影名'].values\n",
    "                hit += len(list(set(real_movie_list).intersection(set(predict_movie_list))))\n",
    "                predict_watched += len(list(set(predict_movie_list)))\n",
    "                real_watched += len(list(set(real_movie_list)))\n",
    "\n",
    "    precision =  hit / (1.0 * predict_watched)\n",
    "    recall =  hit/(1.0* real_watched)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return precision,recall,f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "    '用户ID', 'user_rating_avg', 'user_rating_max', 'user_rating_min',\n",
    "       'user_rating_median', '评分', '电影名', '用户评论次数_观看电影个数',\n",
    "    '类型',  '地区',\n",
    "       '导演', '特色', '豆瓣网评分'\n",
    "]\n",
    "\n",
    "LABEL_COLUMN = \"喜欢\"\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    \"类型\", \"特色\",\"地区\", \"导演\",\"电影名\"\n",
    "]\n",
    "\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    \"用户ID\", \"user_rating_avg\", \"user_rating_max\", \"user_rating_min\",\n",
    "    \"user_rating_median\",\"用户评论次数_观看电影个数\",\"豆瓣网评分\"\n",
    "]\n",
    "\n",
    "my_dir = 'D:/python/My_Project/'\n",
    "data_dir = 'D:/python/Jupyter_Last_project/dataset/'\n",
    "action_cate8_path = my_dir + '/cache/actions_train.pkl'\n",
    "allr_df = get_all_real_datas_change().copy(deep=True)\n",
    "offline_test()\n",
    "p,r,f1 = PrecisonRecall()\n",
    "print('p is\\n',p)\n",
    "print('r is\\n',r)\n",
    "print('f1 is\\n',f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
