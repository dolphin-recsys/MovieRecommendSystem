{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movies_dataset():\n",
    "    movie_data = pd.read_csv(data_dir + './dataset1/movie.csv')\n",
    "    movie_data = movie_data.rename(columns = {'评分': \"豆瓣网评分\"})\n",
    "    return movie_data \n",
    "\n",
    "def load_user_and_ratings() :\n",
    "    user_data = pd.read_csv(data_dir + './dataset1/user.csv')\n",
    "    #user_data['评论时间'] = pd.to_datetime(user_data['评论时间'])   \n",
    "    return user_data \n",
    "\n",
    "def load_usercomments():\n",
    "    comment_data = pd.read_csv(data_dir + './DMSC.csv')\n",
    "    return comment_data \n",
    "\n",
    "\n",
    "data_dir = 'D:/python/Jupyter_Last_project/dataset/'\n",
    "save_dir = 'D:/python/My_Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>类型</th>\n",
       "      <th>主演</th>\n",
       "      <th>地区</th>\n",
       "      <th>导演</th>\n",
       "      <th>特色</th>\n",
       "      <th>豆瓣网评分</th>\n",
       "      <th>电影名</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>剧情</td>\n",
       "      <td>徐峥|王传君|周一围|谭卓|章宇</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>文牧野</td>\n",
       "      <td>经典</td>\n",
       "      <td>8.9</td>\n",
       "      <td>我不是药神</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>剧情</td>\n",
       "      <td>冯小刚|许晴|张涵予|刘桦|李易峰</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>管虎</td>\n",
       "      <td>经典</td>\n",
       "      <td>7.8</td>\n",
       "      <td>老炮儿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>剧情</td>\n",
       "      <td>王宝强|刘昊然|肖央|刘承羽|尚语贤</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>陈思诚</td>\n",
       "      <td>经典</td>\n",
       "      <td>6.7</td>\n",
       "      <td>唐人街探案2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>剧情</td>\n",
       "      <td>任素汐|大力|刘帅良|裴魁山|阿如那</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>周申|刘露</td>\n",
       "      <td>经典</td>\n",
       "      <td>8.3</td>\n",
       "      <td>驴得水</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>剧情</td>\n",
       "      <td>徐峥|王宝强|李曼|李小璐|左小青</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>叶伟民</td>\n",
       "      <td>经典</td>\n",
       "      <td>7.5</td>\n",
       "      <td>人在囧途</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   类型                  主演    地区     导演  特色  豆瓣网评分     电影名\n",
       "0  剧情    徐峥|王传君|周一围|谭卓|章宇  中国大陆    文牧野  经典    8.9   我不是药神\n",
       "1  剧情   冯小刚|许晴|张涵予|刘桦|李易峰  中国大陆     管虎  经典    7.8     老炮儿\n",
       "2  剧情  王宝强|刘昊然|肖央|刘承羽|尚语贤  中国大陆    陈思诚  经典    6.7  唐人街探案2\n",
       "3  剧情  任素汐|大力|刘帅良|裴魁山|阿如那  中国大陆  周申|刘露  经典    8.3     驴得水\n",
       "4  剧情   徐峥|王宝强|李曼|李小璐|左小青  中国大陆    叶伟民  经典    7.5    人在囧途"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_data = load_movies_dataset()\n",
    "movies_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>评分</th>\n",
       "      <th>用户名</th>\n",
       "      <th>评论时间</th>\n",
       "      <th>用户ID</th>\n",
       "      <th>电影名</th>\n",
       "      <th>类型</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>身似</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>1</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>爱情</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>有意识的贱民</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>3</td>\n",
       "      <td>战争的恐怖</td>\n",
       "      <td>战争</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>亿万露电</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>4</td>\n",
       "      <td>豪勇七蛟龙</td>\n",
       "      <td>战争</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Marni</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>5</td>\n",
       "      <td>无序之主</td>\n",
       "      <td>犯罪</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>马西嘻嘻嘻</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>6</td>\n",
       "      <td>时装店风波</td>\n",
       "      <td>同性</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   评分     用户名                 评论时间  用户ID    电影名  类型\n",
       "0   2      身似  2018-01-05 15:05:06     1   心雨花露  爱情\n",
       "1   4  有意识的贱民  2018-01-05 15:05:06     3  战争的恐怖  战争\n",
       "2   2    亿万露电  2018-01-05 15:05:06     4  豪勇七蛟龙  战争\n",
       "3   2   Marni  2018-01-05 15:05:06     5   无序之主  犯罪\n",
       "4   4   马西嘻嘻嘻  2018-01-05 15:05:06     6  时装店风波  同性"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_data = load_user_and_ratings()\n",
    "users_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188843 entries, 0 to 188842\n",
      "Data columns (total 6 columns):\n",
      "评分      188843 non-null int64\n",
      "用户名     188843 non-null object\n",
      "评论时间    188843 non-null object\n",
      "用户ID    188843 non-null int64\n",
      "电影名     188843 non-null object\n",
      "类型      188843 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 8.6+ MB\n"
     ]
    }
   ],
   "source": [
    "users_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13545"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_data.用户ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13532"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_data.用户名.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看用户数据和用户评论数据的交集由于评论数据中电影个数过少。不打算使用评论数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Movie_Name_EN</th>\n",
       "      <th>Movie_Name_CN</th>\n",
       "      <th>Crawl_Date</th>\n",
       "      <th>Number</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Star</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>然潘</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>3</td>\n",
       "      <td>连奥创都知道整容要去韩国。</td>\n",
       "      <td>2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>2</td>\n",
       "      <td>更深的白色</td>\n",
       "      <td>2015-04-24</td>\n",
       "      <td>2</td>\n",
       "      <td>非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>3</td>\n",
       "      <td>有意识的贱民</td>\n",
       "      <td>2015-04-26</td>\n",
       "      <td>2</td>\n",
       "      <td>2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>4</td>\n",
       "      <td>不老的李大爷耶</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>4</td>\n",
       "      <td>《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>5</td>\n",
       "      <td>ZephyrO</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>2</td>\n",
       "      <td>虽然从头打到尾，但是真的很无聊啊。</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>6</td>\n",
       "      <td>同志亦凡人中文站</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>3</td>\n",
       "      <td>剧情不如第一集好玩了，全靠密集笑点在提神。僧多粥少的直接后果就是每部寡姐都要换着队友谈恋爱...</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>7</td>\n",
       "      <td>Danny</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>2</td>\n",
       "      <td>只有一颗彩蛋必须降一星。外加漫威的编剧是有心无力了吧。复仇者联盟只能永远着手与团队的和与不...</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>8</td>\n",
       "      <td>gYroS</td>\n",
       "      <td>2015-04-28</td>\n",
       "      <td>2</td>\n",
       "      <td>看腻了这些打来打去的烂片</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>9</td>\n",
       "      <td>tidd熊</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>3</td>\n",
       "      <td>漫威粉勿喷，真感觉比第一部差了些火候。没想到奥创竟然这么弱，出来失望之极。。。</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Avengers Age of Ultron</td>\n",
       "      <td>复仇者联盟2</td>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>10</td>\n",
       "      <td>桃桃淘电影</td>\n",
       "      <td>2015-05-12</td>\n",
       "      <td>3</td>\n",
       "      <td>属于超级英雄的春晚，角色如走马灯一样出场。眼花缭乱的同时却没办法给人留下太深印象。这部内容...</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID           Movie_Name_EN Movie_Name_CN  Crawl_Date  Number  Username  \\\n",
       "0   0  Avengers Age of Ultron        复仇者联盟2  2017-01-22       1        然潘   \n",
       "1   1  Avengers Age of Ultron        复仇者联盟2  2017-01-22       2     更深的白色   \n",
       "2   2  Avengers Age of Ultron        复仇者联盟2  2017-01-22       3    有意识的贱民   \n",
       "3   3  Avengers Age of Ultron        复仇者联盟2  2017-01-22       4   不老的李大爷耶   \n",
       "4   4  Avengers Age of Ultron        复仇者联盟2  2017-01-22       5   ZephyrO   \n",
       "5   5  Avengers Age of Ultron        复仇者联盟2  2017-01-22       6  同志亦凡人中文站   \n",
       "6   6  Avengers Age of Ultron        复仇者联盟2  2017-01-22       7     Danny   \n",
       "7   7  Avengers Age of Ultron        复仇者联盟2  2017-01-22       8     gYroS   \n",
       "8   8  Avengers Age of Ultron        复仇者联盟2  2017-01-22       9     tidd熊   \n",
       "9   9  Avengers Age of Ultron        复仇者联盟2  2017-01-22      10     桃桃淘电影   \n",
       "\n",
       "         Date  Star                                            Comment  Like  \n",
       "0  2015-05-13     3                                      连奥创都知道整容要去韩国。  2404  \n",
       "1  2015-04-24     2   非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...  1231  \n",
       "2  2015-04-26     2   2015年度最失望作品。以为面面俱到，实则画蛇添足；以为主题深刻，实则老调重弹；以为推陈出...  1052  \n",
       "3  2015-04-23     4   《铁人2》中勾引钢铁侠，《妇联1》中勾引鹰眼，《美队2》中勾引美国队长，在《妇联2》中终于...  1045  \n",
       "4  2015-04-22     2                                  虽然从头打到尾，但是真的很无聊啊。   723  \n",
       "5  2015-04-22     3   剧情不如第一集好玩了，全靠密集笑点在提神。僧多粥少的直接后果就是每部寡姐都要换着队友谈恋爱...   671  \n",
       "6  2015-04-23     2   只有一颗彩蛋必须降一星。外加漫威的编剧是有心无力了吧。复仇者联盟只能永远着手与团队的和与不...   641  \n",
       "7  2015-04-28     2                                       看腻了这些打来打去的烂片   576  \n",
       "8  2015-04-23     3            漫威粉勿喷，真感觉比第一部差了些火候。没想到奥创竟然这么弱，出来失望之极。。。   481  \n",
       "9  2015-05-12     3   属于超级英雄的春晚，角色如走马灯一样出场。眼花缭乱的同时却没办法给人留下太深印象。这部内容...   443  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_data = load_usercomments()\n",
    "comments_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_usersa =  users_data.用户名.unique() \n",
    "# list_user_comments =  comments_data.Username.unique() \n",
    "\n",
    "# res = []\n",
    "# for i in list_usersa:\n",
    "#     if i in list_user_comments:\n",
    "#         res.append(i)\n",
    "# print('交集\\n',len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in list_usersa:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 得到所有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>评分</th>\n",
       "      <th>用户名</th>\n",
       "      <th>评论时间</th>\n",
       "      <th>用户ID</th>\n",
       "      <th>电影名</th>\n",
       "      <th>类型</th>\n",
       "      <th>主演</th>\n",
       "      <th>地区</th>\n",
       "      <th>导演</th>\n",
       "      <th>特色</th>\n",
       "      <th>豆瓣网评分</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>身似</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>1</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>爱情</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>青春</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>身似</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>1</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>爱情</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>励志</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>缘三</td>\n",
       "      <td>2018-01-08 12:24:36</td>\n",
       "      <td>7884</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>爱情</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>青春</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>缘三</td>\n",
       "      <td>2018-01-08 12:24:36</td>\n",
       "      <td>7884</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>爱情</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>励志</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Moss大妖</td>\n",
       "      <td>2018-01-18 13:30:05</td>\n",
       "      <td>4919</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>爱情</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>中国大陆</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>青春</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   评分     用户名                 评论时间  用户ID   电影名  类型   主演    地区   导演  特色  豆瓣网评分\n",
       "0   2      身似  2018-01-05 15:05:06     1  心雨花露  爱情  姬晨牧  中国大陆  喻瀚湫  青春    6.4\n",
       "1   2      身似  2018-01-05 15:05:06     1  心雨花露  爱情  姬晨牧  中国大陆  喻瀚湫  励志    6.4\n",
       "2  10      缘三  2018-01-08 12:24:36  7884  心雨花露  爱情  姬晨牧  中国大陆  喻瀚湫  青春    6.4\n",
       "3  10      缘三  2018-01-08 12:24:36  7884  心雨花露  爱情  姬晨牧  中国大陆  喻瀚湫  励志    6.4\n",
       "4   8  Moss大妖  2018-01-18 13:30:05  4919  心雨花露  爱情  姬晨牧  中国大陆  喻瀚湫  青春    6.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_data = pd.merge(users_data,movies_data,on=['电影名','类型'])\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253054, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.类型.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.地区.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_data.特色.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 类型、地区、特色值比较少，可以使用get_dummies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicts(datain,column_name):\n",
    "    if column_name not in ['类型','特色','地区']:\n",
    "        print('所传列名必须是类型，特色，地区')\n",
    "        return None\n",
    "    list_type = datain[column_name].unique()\n",
    "    list_typeindex = [i for i in range(len(list_type))]\n",
    "    dict_typetoindex = dict(zip(list_type,list_typeindex))\n",
    "    print('dict_typetoindex\\n',dict_typetoindex)\n",
    "    dict_indextotype = dict(zip(list_typeindex,list_type))\n",
    "    print('dict_indextotype\\n',dict_indextotype)\n",
    "    return dict_typetoindex,dict_indextotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_dicts() missing 1 required positional argument: 'column_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0b6326bd7a58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdict_typetoindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'类型'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdict_featuretoindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'特色'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdict_areatoindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'地区'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'类型'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'类型'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mdict_typetoindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'特色'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'特色'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mdict_featuretoindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_dicts() missing 1 required positional argument: 'column_name'"
     ]
    }
   ],
   "source": [
    "dict_typetoindex,_ = get_dicts('类型')\n",
    "dict_featuretoindex,_ = get_dicts('特色')\n",
    "dict_areatoindex,_ = get_dicts('地区')\n",
    "all_data['类型'] = all_data['类型'].map(lambda x :dict_typetoindex[x])\n",
    "all_data['特色'] = all_data['特色'].map(lambda x :dict_featuretoindex[x])\n",
    "all_data['地区'] = all_data['地区'].map(lambda x :dict_areatoindex[x])\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.导演.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20509"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.主演.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 借鉴JD_solution_one先做出类型地区特色的一天时间窗口的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pickle, os\n",
    "import sys\n",
    "sys.path.append(r'D:/python/My_Project/')\n",
    "sys.path.append(\"..\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_path = save_dir + '/cache/all_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gen_action(start_date, end_date):\n",
    "    \"\"\"\n",
    "    产生指定时间区间的用户评论数据\n",
    "    \"\"\"\n",
    "    if os.path.exists(all_data_path):\n",
    "        actions = pickle.load(open(all_data_path, 'rb'))\n",
    "    else:\n",
    "        movies_data = load_movies_dataset()       \n",
    "        users_data = load_user_and_ratings()\n",
    "        print('gen_action users_data info\\n',users_data.info )\n",
    "        actions = pd.merge(users_data,movies_data,on=['电影名','类型'])\n",
    "        dict_typetoindex,_ = get_dicts(actions,'类型')\n",
    "        dict_featuretoindex,_ = get_dicts(actions,'特色')\n",
    "        dict_areatoindex,_ = get_dicts(actions,'地区')\n",
    "        print('actions 类型  \\n',actions['类型'].value_counts())\n",
    "        actions['类型'] = actions['类型'].map(lambda x :dict_typetoindex[x])\n",
    "        actions['特色'] = actions['特色'].map(lambda x :dict_featuretoindex[x])\n",
    "        actions['地区'] = actions['地区'].map(lambda x :dict_areatoindex[x])\n",
    "        pickle.dump(actions, open(all_data_path, 'wb'))\n",
    "    actions = actions[(actions.评论时间 >= start_date) & (actions.评论时间 < end_date)]\n",
    "    return actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample(end_date, span=5):\n",
    "    \"\"\"\n",
    "    产生训练、测试结束日前n天的交互用户ID和电影名\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=span)\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    dump_path = './cache/samples/samples_%s_%s.pkl' % (start_date, end_date)\n",
    "    if os.path.exists(dump_path):\n",
    "        samples = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        actions = gen_action(start_date, end_date)\n",
    "        samples = actions[['用户ID','电影名','评分']].drop_duplicates()\n",
    "        pickle.dump(actions, open(dump_path, 'wb'))\n",
    "    return samples\n",
    "\n",
    "#将评分 >= 6的用户标记为1\n",
    "def gen_labels(start_date, end_date, span=5):\n",
    "    \"\"\"\n",
    "    产生交互日区间内的购买情况\n",
    "    \"\"\"\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=span)\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    dump_path = './cache/labels/labels_%s_%s.pkl' % (start_date, end_date)\n",
    "    if os.path.exists(dump_path):\n",
    "        labels = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        actions = gen_action(start_date, end_date)\n",
    "        actions = actions[['用户ID','电影名','评分']].drop_duplicates()\n",
    "        actions['喜欢']  = actions['评分'].map(lambda x : 1 if x >=6 else 0)   \n",
    "        labels = actions[['用户ID','电影名','喜欢']]\n",
    "        pickle.dump(labels, open(dump_path, 'wb'))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_action_feat(start_date, end_date, span):\n",
    "    \"\"\"\n",
    "    指定区间内的用户电影交互特征(按天窗口）\n",
    "    \"\"\"\n",
    "    dump_path = './cache/user_feature/user_action_feat_%s_%s.pkl' % (start_date, end_date)\n",
    "    if os.path.exists(dump_path):\n",
    "        actions = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        actions = gen_action(start_date, end_date)\n",
    "        actions_feature = actions.copy(deep=True)#特色\n",
    "        actions_area = actions.copy(deep=True)#地区\n",
    "        active_days_hour = actions[['用户ID', '评论时间']]#观看天数#活跃天数\n",
    "        active_days_day = actions[['用户ID', '评论时间']]#观看天数#活跃天数\n",
    "        product_nums = actions[['用户ID', '电影名']]#1天内用户观看电影的个数\n",
    "\n",
    "        #1天内特征\n",
    "        if span == 1 :\n",
    "            #用户span天内 在那种类型的电影上有过评论 \n",
    "            df = pd.get_dummies(actions['类型'], prefix='%d_day_user_type' % (span))\n",
    "            actions_sku = pd.concat([actions[['用户ID', '电影名']], df], axis=1)\n",
    "            actions_sku = actions_sku.groupby(['用户ID', '电影名'], as_index=False).sum()\n",
    "            del actions_sku['电影名']\n",
    "            actions_sku['%d_day_user_type_sum' % (span)] = \\\n",
    "                actions_sku['%d_day_user_type_0' % (span)] + \\\n",
    "                actions_sku['%d_day_user_type_1' % (span)] + actions_sku['%d_day_user_type_2' % (span)] + actions_sku['%d_day_user_type_3' % (span)] + \\\n",
    "                actions_sku['%d_day_user_type_4' % (span)] + actions_sku['%d_day_user_type_5' % (span)] + actions_sku['%d_day_user_type_6' % (span)] + \\\n",
    "                actions_sku['%d_day_user_type_7' % (span)] + actions_sku['%d_day_user_type_8' % (span)] + actions_sku['%d_day_user_type_9' % (span)] + \\\n",
    "                actions_sku['%d_day_user_type_10' % (span)] + actions_sku['%d_day_user_type_11' % (span)] + actions_sku['%d_day_user_type_12' % (span)] + \\\n",
    "                actions_sku['%d_day_user_type_13' % (span)] + actions_sku['%d_day_user_type_14' % (span)] + actions_sku['%d_day_user_type_15' % (span)] + \\\n",
    "                actions_sku['%d_day_user_type_16' % (span)] + actions_sku['%d_day_user_type_17' % (span)] + actions_sku['%d_day_user_type_18' % (span)] \n",
    "            actions_mean = actions_sku.groupby(['用户ID'], as_index=False).mean()\n",
    "            actions_mean.rename(columns={'%d_day_user_type_0' % (span): '%d_day_user_type_0_avg' % (span),\n",
    "                                         '%d_day_user_type_1' % (span): '%d_day_user_type_1_avg' % (span),\n",
    "                                         '%d_day_user_type_2' % (span): '%d_day_user_type_2_avg' % (span),\n",
    "                                         '%d_day_user_type_3' % (span): '%d_day_user_type_3_avg' % (span),\n",
    "                                         '%d_day_user_type_4' % (span): '%d_day_user_type_4_avg' % (span),\n",
    "                                         '%d_day_user_type_5' % (span): '%d_day_user_type_5_avg' % (span),\n",
    "                                         '%d_day_user_type_6' % (span): '%d_day_user_type_6_avg' % (span),\n",
    "                                         '%d_day_user_type_7' % (span): '%d_day_user_type_7_avg' % (span),\n",
    "                                         '%d_day_user_type_8' % (span): '%d_day_user_type_8_avg' % (span),\n",
    "                                         '%d_day_user_type_9' % (span): '%d_day_user_type_9_avg' % (span),\n",
    "                                         '%d_day_user_type_10' % (span): '%d_day_user_type_10_avg' % (span),\n",
    "                                         '%d_day_user_type_11' % (span): '%d_day_user_type_11_avg' % (span),\n",
    "                                         '%d_day_user_type_12' % (span): '%d_day_user_type_12_avg' % (span),\n",
    "                                         '%d_day_user_type_13' % (span): '%d_day_user_type_13_avg' % (span),\n",
    "                                         '%d_day_user_type_14' % (span): '%d_day_user_type_14_avg' % (span),\n",
    "                                         '%d_day_user_type_15' % (span): '%d_day_user_type_15_avg' % (span),\n",
    "                                         '%d_day_user_type_16' % (span): '%d_day_user_type_16_avg' % (span),\n",
    "                                         '%d_day_user_type_17' % (span): '%d_day_user_type_17_avg' % (span),\n",
    "                                         '%d_day_user_type_18' % (span): '%d_day_user_type_18_avg' % (span),\n",
    "                                         '%d_day_user_type_sum' % (span): '%d_day_user_type_avg' % (span)}, inplace=True)\n",
    "            actions_max = actions_sku.groupby(['用户ID'], as_index=False).max()\n",
    "            actions_max.rename(columns={ '%d_day_user_type_0' % (span): '%d_day_user_type_0_max' % (span),\n",
    "                                         '%d_day_user_type_1' % (span): '%d_day_user_type_1_max' % (span),\n",
    "                                         '%d_day_user_type_2' % (span): '%d_day_user_type_2_max' % (span),\n",
    "                                         '%d_day_user_type_3' % (span): '%d_day_user_type_3_max' % (span),\n",
    "                                         '%d_day_user_type_4' % (span): '%d_day_user_type_4_max' % (span),\n",
    "                                         '%d_day_user_type_5' % (span): '%d_day_user_type_5_max' % (span),\n",
    "                                         '%d_day_user_type_6' % (span): '%d_day_user_type_6_max' % (span),\n",
    "                                         '%d_day_user_type_7' % (span): '%d_day_user_type_7_max' % (span),\n",
    "                                         '%d_day_user_type_8' % (span): '%d_day_user_type_8_max' % (span),\n",
    "                                         '%d_day_user_type_9' % (span): '%d_day_user_type_9_max' % (span),\n",
    "                                         '%d_day_user_type_10' % (span): '%d_day_user_type_10_max' % (span),\n",
    "                                         '%d_day_user_type_11' % (span): '%d_day_user_type_11_max' % (span),\n",
    "                                         '%d_day_user_type_12' % (span): '%d_day_user_type_12_max' % (span),\n",
    "                                         '%d_day_user_type_13' % (span): '%d_day_user_type_13_max' % (span),\n",
    "                                         '%d_day_user_type_14' % (span): '%d_day_user_type_14_max' % (span),\n",
    "                                         '%d_day_user_type_15' % (span): '%d_day_user_type_15_max' % (span),\n",
    "                                         '%d_day_user_type_16' % (span): '%d_day_user_type_16_max' % (span),\n",
    "                                         '%d_day_user_type_17' % (span): '%d_day_user_type_17_max' % (span),\n",
    "                                         '%d_day_user_type_18' % (span): '%d_day_user_type_18_max' % (span),\n",
    "                                         '%d_day_user_type_sum' % (span): '%d_day_user_type_max' % (span)}, inplace=True)\n",
    "            actions_min = actions_sku.groupby(['用户ID'], as_index=False).min()\n",
    "            actions_min.rename(columns={'%d_day_user_type_0' % (span): '%d_day_user_type_0_min' % (span),\n",
    "                                         '%d_day_user_type_1' % (span): '%d_day_user_type_1_min' % (span),\n",
    "                                         '%d_day_user_type_2' % (span): '%d_day_user_type_2_min' % (span),\n",
    "                                         '%d_day_user_type_3' % (span): '%d_day_user_type_3_min' % (span),\n",
    "                                         '%d_day_user_type_4' % (span): '%d_day_user_type_4_min' % (span),\n",
    "                                         '%d_day_user_type_5' % (span): '%d_day_user_type_5_min' % (span),\n",
    "                                         '%d_day_user_type_6' % (span): '%d_day_user_type_6_min' % (span),\n",
    "                                         '%d_day_user_type_7' % (span): '%d_day_user_type_7_min' % (span),\n",
    "                                         '%d_day_user_type_8' % (span): '%d_day_user_type_8_min' % (span),\n",
    "                                         '%d_day_user_type_9' % (span): '%d_day_user_type_9_min' % (span),\n",
    "                                         '%d_day_user_type_10' % (span): '%d_day_user_type_10_min' % (span),\n",
    "                                         '%d_day_user_type_11' % (span): '%d_day_user_type_11_min' % (span),\n",
    "                                         '%d_day_user_type_12' % (span): '%d_day_user_type_12_min' % (span),\n",
    "                                         '%d_day_user_type_13' % (span): '%d_day_user_type_13_min' % (span),\n",
    "                                         '%d_day_user_type_14' % (span): '%d_day_user_type_14_min' % (span),\n",
    "                                         '%d_day_user_type_15' % (span): '%d_day_user_type_15_min' % (span),\n",
    "                                         '%d_day_user_type_16' % (span): '%d_day_user_type_16_min' % (span),\n",
    "                                         '%d_day_user_type_17' % (span): '%d_day_user_type_17_min' % (span),\n",
    "                                         '%d_day_user_type_18' % (span): '%d_day_user_type_18_min' % (span),\n",
    "                                         '%d_day_user_type_sum' % (span): '%d_day_user_type_min' % (span)}, inplace=True)\n",
    "            actions_median = actions_sku.groupby(['用户ID'], as_index=False).median()\n",
    "            actions_median.rename(columns={\n",
    "                                        '%d_day_user_type_0' % (span): '%d_day_user_type_0_median' % (span),\n",
    "                                         '%d_day_user_type_1' % (span): '%d_day_user_type_1_median' % (span),\n",
    "                                         '%d_day_user_type_2' % (span): '%d_day_user_type_2_median' % (span),\n",
    "                                         '%d_day_user_type_3' % (span): '%d_day_user_type_3_median' % (span),\n",
    "                                         '%d_day_user_type_4' % (span): '%d_day_user_type_4_median' % (span),\n",
    "                                         '%d_day_user_type_5' % (span): '%d_day_user_type_5_median' % (span),\n",
    "                                         '%d_day_user_type_6' % (span): '%d_day_user_type_6_median' % (span),\n",
    "                                         '%d_day_user_type_7' % (span): '%d_day_user_type_7_median' % (span),\n",
    "                                         '%d_day_user_type_8' % (span): '%d_day_user_type_8_median' % (span),\n",
    "                                         '%d_day_user_type_9' % (span): '%d_day_user_type_9_median' % (span),\n",
    "                                         '%d_day_user_type_10' % (span): '%d_day_user_type_10_median' % (span),\n",
    "                                         '%d_day_user_type_11' % (span): '%d_day_user_type_11_median' % (span),\n",
    "                                         '%d_day_user_type_12' % (span): '%d_day_user_type_12_median' % (span),\n",
    "                                         '%d_day_user_type_13' % (span): '%d_day_user_type_13_median' % (span),\n",
    "                                         '%d_day_user_type_14' % (span): '%d_day_user_type_14_median' % (span),\n",
    "                                         '%d_day_user_type_15' % (span): '%d_day_user_type_15_median' % (span),\n",
    "                                         '%d_day_user_type_16' % (span): '%d_day_user_type_16_median' % (span),\n",
    "                                         '%d_day_user_type_17' % (span): '%d_day_user_type_17_median' % (span),\n",
    "                                         '%d_day_user_type_18' % (span): '%d_day_user_type_18_median' % (span),\n",
    "                                         '%d_day_user_type_sum' % (span): '%d_day_user_type_median' % (span)}, inplace=True)\n",
    "            \n",
    "            # 交互总量(一个用户在span天内评论的电影的总个数)\n",
    "            df = pd.get_dummies(actions['类型'], prefix='%d_day_user_type' % (span))\n",
    "            actions = pd.concat([actions['用户ID'], df], axis=1)  # type: pd.DataFrame\n",
    "            actions = actions.groupby(['用户ID'], as_index=False).sum()\n",
    "            \n",
    "            #merge \n",
    "            actions = pd.merge(actions, actions_median, how='left', on='用户ID')\n",
    "            actions = pd.merge(actions, actions_max, how='left', on='用户ID')\n",
    "            actions = pd.merge(actions, actions_min, how='left', on='用户ID')\n",
    "            actions = pd.merge(actions, actions_mean, how='left', on='用户ID')\n",
    "            \n",
    "            #特色\n",
    "            df = pd.get_dummies(actions_feature['特色'], prefix='%d_day_user_feature' % (span))\n",
    "            actions_feature_sku = pd.concat([actions_feature[['用户ID', '电影名']], df], axis=1)\n",
    "            actions_feature_sku = actions_feature_sku.groupby(['用户ID', '电影名'], as_index=False).sum()\n",
    "            del actions_feature_sku['电影名']\n",
    "            actions_feature_sku['%d_day_user_feature_sum' % (span)] = \\\n",
    "                actions_feature_sku['%d_day_user_feature_0' % (span)] + \\\n",
    "                actions_feature_sku['%d_day_user_feature_1' % (span)] + actions_feature_sku['%d_day_user_feature_2' % (span)] + actions_feature_sku['%d_day_user_feature_3' % (span)] + \\\n",
    "                actions_feature_sku['%d_day_user_feature_4' % (span)] + actions_feature_sku['%d_day_user_feature_5' % (span)] + actions_feature_sku['%d_day_user_feature_6' % (span)] + \\\n",
    "                actions_feature_sku['%d_day_user_feature_7' % (span)] + actions_feature_sku['%d_day_user_feature_8' % (span)]\n",
    "            actions_feature_mean = actions_feature_sku.groupby(['用户ID'], as_index=False).mean()\n",
    "            actions_feature_mean.rename(columns={'%d_day_user_feature_0' % (span): '%d_day_user_feature_0_avg' % (span),\n",
    "                                         '%d_day_user_feature_1' % (span): '%d_day_user_feature_1_avg' % (span),\n",
    "                                         '%d_day_user_feature_2' % (span): '%d_day_user_feature_2_avg' % (span),\n",
    "                                         '%d_day_user_feature_3' % (span): '%d_day_user_feature_3_avg' % (span),\n",
    "                                         '%d_day_user_feature_4' % (span): '%d_day_user_feature_4_avg' % (span),\n",
    "                                         '%d_day_user_feature_5' % (span): '%d_day_user_feature_5_avg' % (span),\n",
    "                                         '%d_day_user_feature_6' % (span): '%d_day_user_feature_6_avg' % (span),\n",
    "                                         '%d_day_user_feature_7' % (span): '%d_day_user_feature_7_avg' % (span),\n",
    "                                         '%d_day_user_feature_8' % (span): '%d_day_user_feature_8_avg' % (span),\n",
    "                                         '%d_day_user_feature_sum' % (span): '%d_day_user_feature_avg' % (span)}, inplace=True)\n",
    "            actions_feature_max = actions_feature_sku.groupby(['用户ID'], as_index=False).max()\n",
    "            actions_feature_max.rename(columns={ '%d_day_user_feature_0' % (span): '%d_day_user_feature_0_max' % (span),\n",
    "                                         '%d_day_user_feature_1' % (span): '%d_day_user_feature_1_max' % (span),\n",
    "                                         '%d_day_user_feature_2' % (span): '%d_day_user_feature_2_max' % (span),\n",
    "                                         '%d_day_user_feature_3' % (span): '%d_day_user_feature_3_max' % (span),\n",
    "                                         '%d_day_user_feature_4' % (span): '%d_day_user_feature_4_max' % (span),\n",
    "                                         '%d_day_user_feature_5' % (span): '%d_day_user_feature_5_max' % (span),\n",
    "                                         '%d_day_user_feature_6' % (span): '%d_day_user_feature_6_max' % (span),\n",
    "                                         '%d_day_user_feature_7' % (span): '%d_day_user_feature_7_max' % (span),\n",
    "                                         '%d_day_user_feature_8' % (span): '%d_day_user_feature_8_max' % (span),\n",
    "                                         '%d_day_user_feature_sum' % (span): '%d_day_user_feature_max' % (span)}, inplace=True)\n",
    "            actions_feature_min = actions_feature_sku.groupby(['用户ID'], as_index=False).min()\n",
    "            actions_feature_min.rename(columns={'%d_day_user_feature_0' % (span): '%d_day_user_feature_0_min' % (span),\n",
    "                                         '%d_day_user_feature_1' % (span): '%d_day_user_feature_1_min' % (span),\n",
    "                                         '%d_day_user_feature_2' % (span): '%d_day_user_feature_2_min' % (span),\n",
    "                                         '%d_day_user_feature_3' % (span): '%d_day_user_feature_3_min' % (span),\n",
    "                                         '%d_day_user_feature_4' % (span): '%d_day_user_feature_4_min' % (span),\n",
    "                                         '%d_day_user_feature_5' % (span): '%d_day_user_feature_5_min' % (span),\n",
    "                                         '%d_day_user_feature_6' % (span): '%d_day_user_feature_6_min' % (span),\n",
    "                                         '%d_day_user_feature_7' % (span): '%d_day_user_feature_7_min' % (span),\n",
    "                                         '%d_day_user_feature_8' % (span): '%d_day_user_feature_8_min' % (span),\n",
    "                                         '%d_day_user_feature_sum' % (span): '%d_day_user_feature_min' % (span)}, inplace=True)\n",
    "            actions_feature_median = actions_feature_sku.groupby(['用户ID'], as_index=False).median()\n",
    "            actions_feature_median.rename(columns={\n",
    "                                        '%d_day_user_feature_0' % (span): '%d_day_user_feature_0_median' % (span),\n",
    "                                         '%d_day_user_feature_1' % (span): '%d_day_user_feature_1_median' % (span),\n",
    "                                         '%d_day_user_feature_2' % (span): '%d_day_user_feature_2_median' % (span),\n",
    "                                         '%d_day_user_feature_3' % (span): '%d_day_user_feature_3_median' % (span),\n",
    "                                         '%d_day_user_feature_4' % (span): '%d_day_user_feature_4_median' % (span),\n",
    "                                         '%d_day_user_feature_5' % (span): '%d_day_user_feature_5_median' % (span),\n",
    "                                         '%d_day_user_feature_6' % (span): '%d_day_user_feature_6_median' % (span),\n",
    "                                         '%d_day_user_feature_7' % (span): '%d_day_user_feature_7_median' % (span),\n",
    "                                         '%d_day_user_feature_8' % (span): '%d_day_user_feature_8_median' % (span),\n",
    "                                         '%d_day_user_feature_sum' % (span): '%d_day_user_feature_median' % (span)}, inplace=True)\n",
    "            \n",
    "            # 交互总量(一个用户在span天内评论的电影的总个数)\n",
    "            df = pd.get_dummies(actions_feature['特色'], prefix='%d_day_user_feature' % (span))\n",
    "            actions_feature = pd.concat([actions_feature['用户ID'], df], axis=1)   \n",
    "            actions_feature = actions_feature.groupby(['用户ID'], as_index=False).sum()\n",
    "            \n",
    "            #merge \n",
    "            actions_feature = pd.merge(actions_feature, actions_feature_median, how='left', on='用户ID')\n",
    "            actions_feature = pd.merge(actions_feature, actions_feature_max, how='left', on='用户ID')\n",
    "            actions_feature = pd.merge(actions_feature, actions_feature_min, how='left', on='用户ID')\n",
    "            actions_feature = pd.merge(actions_feature, actions_feature_mean, how='left', on='用户ID')\n",
    "            \n",
    "            #地区\n",
    "            df = pd.get_dummies(actions_area['地区'], prefix='%d_day_user_area' % (span))\n",
    "            print('df columns\\n',df.columns)\n",
    "            actions_area_sku = pd.concat([actions_area[['用户ID', '电影名']], df], axis=1)\n",
    "            actions_area_sku = actions_area_sku.groupby(['用户ID', '电影名'], as_index=False).sum()\n",
    "            del actions_area_sku['电影名']\n",
    "            actions_area_sku['%d_day_user_area_sum' % (span)] = \\\n",
    "                actions_area_sku['%d_day_user_area_0' % (span)] + \\\n",
    "                actions_area_sku['%d_day_user_area_1' % (span)] + actions_area_sku['%d_day_user_area_2' % (span)] + actions_area_sku['%d_day_user_area_3' % (span)] + \\\n",
    "                actions_area_sku['%d_day_user_area_4' % (span)] + actions_area_sku['%d_day_user_area_5' % (span)] + actions_area_sku['%d_day_user_area_6' % (span)] + \\\n",
    "                actions_area_sku['%d_day_user_area_7' % (span)] + actions_area_sku['%d_day_user_area_8' % (span)] + actions_area_sku['%d_day_user_area_9' % (span)]  + \\\n",
    "                actions_area_sku['%d_day_user_area_10' % (span)] + actions_area_sku['%d_day_user_area_11' % (span)] + actions_area_sku['%d_day_user_area_12' % (span)] + \\\n",
    "                actions_area_sku['%d_day_user_area_13' % (span)] + actions_area_sku['%d_day_user_area_14' % (span)] + actions_area_sku['%d_day_user_area_15' % (span)] + \\\n",
    "                actions_area_sku['%d_day_user_area_16' % (span)] + actions_area_sku['%d_day_user_area_17' % (span)] + actions_area_sku['%d_day_user_area_18' % (span)] + \\\n",
    "                actions_area_sku['%d_day_user_area_19' % (span)] + actions_area_sku['%d_day_user_area_20' % (span)]\n",
    "            \n",
    "            actions_area_mean = actions_area_sku.groupby(['用户ID'], as_index=False).mean()\n",
    "            actions_area_mean.rename(columns={'%d_day_user_area_0' % (span): '%d_day_user_area_0_avg' % (span),\n",
    "                                         '%d_day_user_area_1' % (span): '%d_day_user_area_1_avg' % (span),\n",
    "                                         '%d_day_user_area_2' % (span): '%d_day_user_area_2_avg' % (span),\n",
    "                                         '%d_day_user_area_3' % (span): '%d_day_user_area_3_avg' % (span),\n",
    "                                         '%d_day_user_area_4' % (span): '%d_day_user_area_4_avg' % (span),\n",
    "                                         '%d_day_user_area_5' % (span): '%d_day_user_area_5_avg' % (span),\n",
    "                                         '%d_day_user_area_6' % (span): '%d_day_user_area_6_avg' % (span),\n",
    "                                         '%d_day_user_area_7' % (span): '%d_day_user_area_7_avg' % (span),\n",
    "                                         '%d_day_user_area_8' % (span): '%d_day_user_area_8_avg' % (span),\n",
    "                                         '%d_day_user_area_9' % (span): '%d_day_user_area_9_avg' % (span),\n",
    "                                         '%d_day_user_area_10' % (span): '%d_day_user_area_10_avg' % (span),\n",
    "                                         '%d_day_user_area_11' % (span): '%d_day_user_area_11_avg' % (span),\n",
    "                                         '%d_day_user_area_12' % (span): '%d_day_user_area_12_avg' % (span),\n",
    "                                         '%d_day_user_area_13' % (span): '%d_day_user_area_13_avg' % (span),\n",
    "                                         '%d_day_user_area_14' % (span): '%d_day_user_area_14_avg' % (span),\n",
    "                                         '%d_day_user_area_15' % (span): '%d_day_user_area_15_avg' % (span),\n",
    "                                         '%d_day_user_area_16' % (span): '%d_day_user_area_16_avg' % (span),\n",
    "                                          '%d_day_user_area_17' % (span): '%d_day_user_area_17_avg' % (span),\n",
    "                                         '%d_day_user_area_18' % (span): '%d_day_user_area_18_avg' % (span),\n",
    "                                         '%d_day_user_area_19' % (span): '%d_day_user_area_19_avg' % (span),\n",
    "                                         '%d_day_user_area_20' % (span): '%d_day_user_area_20_avg' % (span),\n",
    "                                         '%d_day_user_area_sum' % (span): '%d_day_user_area_avg' % (span)}, inplace=True)\n",
    "            actions_area_max = actions_area_sku.groupby(['用户ID'], as_index=False).max()\n",
    "            actions_area_max.rename(columns={'%d_day_user_area_0' % (span): '%d_day_user_area_0_max' % (span),\n",
    "                                         '%d_day_user_area_1' % (span): '%d_day_user_area_1_max' % (span),\n",
    "                                         '%d_day_user_area_2' % (span): '%d_day_user_area_2_max' % (span),\n",
    "                                         '%d_day_user_area_3' % (span): '%d_day_user_area_3_max' % (span),\n",
    "                                         '%d_day_user_area_4' % (span): '%d_day_user_area_4_max' % (span),\n",
    "                                         '%d_day_user_area_5' % (span): '%d_day_user_area_5_max' % (span),\n",
    "                                         '%d_day_user_area_6' % (span): '%d_day_user_area_6_max' % (span),\n",
    "                                         '%d_day_user_area_7' % (span): '%d_day_user_area_7_max' % (span),\n",
    "                                         '%d_day_user_area_8' % (span): '%d_day_user_area_8_max' % (span),\n",
    "                                        '%d_day_user_area_9' % (span): '%d_day_user_area_9_max' % (span),\n",
    "                                         '%d_day_user_area_10' % (span): '%d_day_user_area_10_max' % (span),\n",
    "                                         '%d_day_user_area_11' % (span): '%d_day_user_area_11_max' % (span),\n",
    "                                         '%d_day_user_area_12' % (span): '%d_day_user_area_12_max' % (span),\n",
    "                                         '%d_day_user_area_13' % (span): '%d_day_user_area_13_max' % (span),\n",
    "                                         '%d_day_user_area_14' % (span): '%d_day_user_area_14_max' % (span),\n",
    "                                         '%d_day_user_area_15' % (span): '%d_day_user_area_15_max' % (span),\n",
    "                                         '%d_day_user_area_16' % (span): '%d_day_user_area_16_max' % (span),\n",
    "                                         '%d_day_user_area_17' % (span): '%d_day_user_area_17_max' % (span),\n",
    "                                         '%d_day_user_area_18' % (span): '%d_day_user_area_18_max' % (span),\n",
    "                                         '%d_day_user_area_19' % (span): '%d_day_user_area_19_max' % (span),\n",
    "                                         '%d_day_user_area_20' % (span): '%d_day_user_area_20_max' % (span),\n",
    "                                         '%d_day_user_area_sum' % (span): '%d_day_user_area_max' % (span)}, inplace=True)\n",
    "            actions_area_min = actions_area_sku.groupby(['用户ID'], as_index=False).min()\n",
    "            actions_area_min.rename(columns={'%d_day_user_area_0' % (span): '%d_day_user_area_0_min' % (span),\n",
    "                                         '%d_day_user_area_1' % (span): '%d_day_user_area_1_min' % (span),\n",
    "                                         '%d_day_user_area_2' % (span): '%d_day_user_area_2_min' % (span),\n",
    "                                         '%d_day_user_area_3' % (span): '%d_day_user_area_3_min' % (span),\n",
    "                                         '%d_day_user_area_4' % (span): '%d_day_user_area_4_min' % (span),\n",
    "                                         '%d_day_user_area_5' % (span): '%d_day_user_area_5_min' % (span),\n",
    "                                         '%d_day_user_area_6' % (span): '%d_day_user_area_6_min' % (span),\n",
    "                                         '%d_day_user_area_7' % (span): '%d_day_user_area_7_min' % (span),\n",
    "                                         '%d_day_user_area_8' % (span): '%d_day_user_area_8_min' % (span),\n",
    "                                         '%d_day_user_area_9' % (span): '%d_day_user_area_9_min' % (span),\n",
    "                                         '%d_day_user_area_10' % (span): '%d_day_user_area_10_min' % (span),\n",
    "                                         '%d_day_user_area_11' % (span): '%d_day_user_area_11_min' % (span),\n",
    "                                         '%d_day_user_area_12' % (span): '%d_day_user_area_12_min' % (span),\n",
    "                                         '%d_day_user_area_13' % (span): '%d_day_user_area_13_min' % (span),\n",
    "                                         '%d_day_user_area_14' % (span): '%d_day_user_area_14_min' % (span),\n",
    "                                         '%d_day_user_area_15' % (span): '%d_day_user_area_15_min' % (span),\n",
    "                                         '%d_day_user_area_16' % (span): '%d_day_user_area_16_min' % (span),\n",
    "                                          '%d_day_user_area_17' % (span): '%d_day_user_area_17_min' % (span),\n",
    "                                         '%d_day_user_area_18' % (span): '%d_day_user_area_18_min' % (span),\n",
    "                                         '%d_day_user_area_19' % (span): '%d_day_user_area_19_min' % (span),\n",
    "                                         '%d_day_user_area_20' % (span): '%d_day_user_area_20_min' % (span),\n",
    "                                         '%d_day_user_area_sum' % (span): '%d_day_user_area_min' % (span)}, inplace=True)\n",
    "            actions_area_median = actions_area_sku.groupby(['用户ID'], as_index=False).median()\n",
    "            actions_area_median.rename(columns={'%d_day_user_area_0' % (span): '%d_day_user_area_0_median' % (span),\n",
    "                                         '%d_day_user_area_1' % (span): '%d_day_user_area_1_median' % (span),\n",
    "                                         '%d_day_user_area_2' % (span): '%d_day_user_area_2_median' % (span),\n",
    "                                         '%d_day_user_area_3' % (span): '%d_day_user_area_3_median' % (span),\n",
    "                                         '%d_day_user_area_4' % (span): '%d_day_user_area_4_median' % (span),\n",
    "                                         '%d_day_user_area_5' % (span): '%d_day_user_area_5_median' % (span),\n",
    "                                         '%d_day_user_area_6' % (span): '%d_day_user_area_6_median' % (span),\n",
    "                                         '%d_day_user_area_7' % (span): '%d_day_user_area_7_median' % (span),\n",
    "                                         '%d_day_user_area_8' % (span): '%d_day_user_area_8_median' % (span),\n",
    "                                          '%d_day_user_area_9' % (span): '%d_day_user_area_9_median' % (span),\n",
    "                                         '%d_day_user_area_10' % (span): '%d_day_user_area_10_median' % (span),\n",
    "                                         '%d_day_user_area_11' % (span): '%d_day_user_area_11_median' % (span),\n",
    "                                         '%d_day_user_area_12' % (span): '%d_day_user_area_12_median' % (span),\n",
    "                                         '%d_day_user_area_13' % (span): '%d_day_user_area_13_median' % (span),\n",
    "                                         '%d_day_user_area_14' % (span): '%d_day_user_area_14_median' % (span),\n",
    "                                         '%d_day_user_area_15' % (span): '%d_day_user_area_15_median' % (span),\n",
    "                                         '%d_day_user_area_16' % (span): '%d_day_user_area_16_median' % (span),\n",
    "                                          '%d_day_user_area_17' % (span): '%d_day_user_area_17_median' % (span),\n",
    "                                         '%d_day_user_area_18' % (span): '%d_day_user_area_18_median' % (span),\n",
    "                                         '%d_day_user_area_19' % (span): '%d_day_user_area_19_median' % (span),\n",
    "                                         '%d_day_user_area_20' % (span): '%d_day_user_area_20_median' % (span),\n",
    "                                         '%d_day_user_area_sum' % (span): '%d_day_user_area_median' % (span)}, inplace=True)\n",
    "            \n",
    "            # 交互总量(一个用户在span天内评论的电影的总个数)\n",
    "            df = pd.get_dummies(actions_area['地区'], prefix='%d_day_user_area' % (span))\n",
    "            actions_area = pd.concat([actions_area['用户ID'], df], axis=1)   \n",
    "            actions_area = actions_area.groupby(['用户ID'], as_index=False).sum()\n",
    "            \n",
    "            #merge \n",
    "            actions_area = pd.merge(actions_area, actions_area_median, how='left', on='用户ID')\n",
    "            actions_area = pd.merge(actions_area, actions_area_max, how='left', on='用户ID')\n",
    "            actions_area = pd.merge(actions_area, actions_area_min, how='left', on='用户ID')\n",
    "            actions_area = pd.merge(actions_area, actions_area_mean, how='left', on='用户ID')\n",
    "            \n",
    "            \n",
    "            #1 天内活动小时数\n",
    "            active_days_hour['评论时间'] = active_days_hour['评论时间'].apply(lambda x: x[11:13])\n",
    "            active_days_hour = active_days_hour.groupby(['用户ID', '评论时间']).size().reset_index()\n",
    "            active_days_hour = active_days_hour.groupby('用户ID').size().reset_index()\n",
    "            active_days_hour.rename(columns={0: '1_day_user_active_hours'}, inplace=True)\n",
    "            \n",
    "            #看过电影个数\n",
    "            product_nums = product_nums.groupby(['用户ID', '电影名']).size().reset_index()\n",
    "            product_nums = product_nums.groupby('用户ID').size().reset_index()\n",
    "            product_nums.rename(columns={0: '1_day_user_movie_nums'}, inplace=True)\n",
    "            \n",
    "            #merge\n",
    "            actions = pd.merge(actions, active_days_hour, how='left', on='用户ID')\n",
    "            actions = pd.merge(actions, product_nums, how='left', on='用户ID')\n",
    "            #merge feature\n",
    "            actions = pd.merge(actions, actions_feature, how='left', on='用户ID')\n",
    "            #merge area\n",
    "            actions = pd.merge(actions, actions_area, how='left', on='用户ID')\n",
    "            \n",
    "            #活动天数\n",
    "            active_days_day['评论时间'] = active_days_day['评论时间'].apply(lambda x: x[0:10])\n",
    "            active_days_day = active_days_day.groupby(['用户ID', '评论时间']).size().reset_index()\n",
    "            active_days_day = active_days_day.groupby('用户ID').size().reset_index()\n",
    "            active_days_day.rename(columns={0: '%d_day_user_active_days' % (span)}, inplace=True)\n",
    "            \n",
    "\n",
    "            #merge \n",
    "            actions = pd.merge(actions, active_days_day, how='left', on='用户ID')\n",
    "            pickle.dump(actions, open(dump_path, 'wb'))\n",
    "\n",
    "    return actions\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_user_action_feat_hour(start_date, end_date, span):\n",
    "    \"\"\"\n",
    "    指定区间内的用户商品交互特征(按小时窗口）\n",
    "    \"\"\"\n",
    "    print('start_date.hour is\\n',start_date.hour)\n",
    "    dump_path = './cache/user_feature/user_action_feat_%sh_%s.pkl' % (start_date.hour, end_date)\n",
    "    if os.path.exists(dump_path):\n",
    "        actions = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        actions = gen_action(str(start_date), end_date)\n",
    "        actions_feature = actions.copy(deep=True)#特色\n",
    "        actions_area = actions.copy(deep=True)#地区\n",
    "        active_days = actions[['用户ID', '评论时间']]\n",
    "\n",
    "        assert actions.shape[0] != 0\n",
    "        #交互总量  --类型\n",
    "        df = pd.get_dummies(actions['类型'], prefix='%d_h_user_type' % (span))\n",
    "        actions = pd.concat([actions['用户ID'], df], axis=1) \n",
    "        actions = actions.groupby(['用户ID'], as_index=False).sum()\n",
    "        #交互总量  --特色 \n",
    "        df = pd.get_dummies(actions_feature['特色'], prefix='%d_h_user_feature' % (span))\n",
    "        actions_feature = pd.concat([actions_feature['用户ID'], df], axis=1) \n",
    "        actions_feature = actions_feature.groupby(['用户ID'], as_index=False).sum()\n",
    "        #交互总量  --地区\n",
    "        df = pd.get_dummies(actions_area['地区'], prefix='%d_h_user_area' % (span))\n",
    "        actions_area = pd.concat([actions_area['用户ID'], df], axis=1) \n",
    "        actions_area = actions_area.groupby(['用户ID'], as_index=False).sum()\n",
    "        #活跃小时数  #统计一个用户在总共几个时段上发生过评论行为\n",
    "        active_days['评论时间'] = active_days['评论时间'].apply(lambda x: x[11:13])\n",
    "        active_days = active_days.groupby(['用户ID', '评论时间']).size().reset_index()\n",
    "        active_days = active_days.groupby('用户ID').size().reset_index()\n",
    "        active_days.rename(columns={0: '%d_h_user_active_hours' % (span)}, inplace=True)\n",
    "        #merge\n",
    "        actions = pd.merge(actions, active_days, how='left', on='用户ID')\n",
    "        actions = pd.merge(actions, actions_feature, how='left', on='用户ID')\n",
    "        actions = pd.merge(actions, actions_area, how='left', on='用户ID')\n",
    "        pickle.dump(actions, open(dump_path, 'wb'))\n",
    "\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_set(train_start_date, train_end_date,  span=30):\n",
    "    \"\"\"\n",
    "    构造训练集\n",
    "    \"\"\"\n",
    "    dump_path = './cache/dataset/train_set_%s_%s.pkl' % (train_start_date, train_end_date)\n",
    "    if os.path.exists(dump_path):\n",
    "        train_set = pickle.load(open(dump_path, 'rb'))\n",
    "    else:\n",
    "        train_set = gen_sample(train_end_date, span)\n",
    "        del train_set['类型']\n",
    "        del train_set['特色']\n",
    "        del train_set['地区']\n",
    "        print('train_set   shape\\n',train_set.shape)     \n",
    "        label = gen_labels(train_start_date, train_end_date)\n",
    "        print('label shape \\n',label.shape)\n",
    "        #window = [1, 2, 3, 5, 7, 10, 15, 21, 30]#天窗特征。1天特征里设计到小时\n",
    "        window = [1]\n",
    "        for i in window:\n",
    "            start_date = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(days=i)\n",
    "            start_date = start_date.strftime('%Y-%m-%d')\n",
    "            user_action_feat = gen_user_action_feat(start_date, train_end_date, i) \n",
    "            #user_action_feat.to_csv('./gen_user_action_feat_day.csv', index=False, index_label=False)\n",
    "            #train_set = pd.merge(train_set, user_action_feat, how='left', on='用户ID')\n",
    "            train_set = pd.merge(train_set, user_action_feat, how='inner', on='用户ID')\n",
    "\n",
    "        #小时窗特征 #如果小时数为4 gen_actions容易产不出特征\n",
    "        window = [8]\n",
    "        for i in window:\n",
    "            start_date = datetime.strptime(train_end_date, '%Y-%m-%d') - timedelta(hours=i)\n",
    "            user_action_feat = gen_user_action_feat_hour(start_date, train_end_date, i)\n",
    "            #train_set = pd.merge(train_set, user_action_feat, how='left', on='用户ID')\n",
    "            train_set = pd.merge(train_set, user_action_feat, how='inner', on='用户ID')\n",
    "        pickle.dump(train_set, open(dump_path, 'wb'))\n",
    "\n",
    "    #1天。8小时。有318个特征\n",
    "    #label 既然使用copy了。标签与时间窗还有什么关系？\n",
    "    label = train_set[['用户ID','电影名','评分']].copy()\n",
    "    \n",
    "    label['喜欢'] =  label['评分'].map(lambda x: 1 if x>=6 else 0)\n",
    "    del label['评分']\n",
    "    del label['用户ID']\n",
    "    del label['电影名']\n",
    "    return train_set,label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "train_start_date = '2018-01-05' train_end_date = '2018-01-30'\n",
    "\n",
    "test_start_date = '2018-01-10' test_end_date = '2018-02-07'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in gen_sample start_date ,end_date is\n",
    " 2017-12-31 20\n",
    " \n",
    " 18-01-30\n",
    " in gen_labels start_date ,end_date is\n",
    " 2018-01-30  2018-02-04 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Dropout, SpatialDropout1D, Activation, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import ReLU, PReLU, LeakyReLU, ELU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(all_real_data,LABEL_COLUMN,CATEGORICAL_COLUMNS,CONTINUOUS_COLUMNS):\n",
    "    print('-----------in preprocessing -----\\n')\n",
    "    print('-----------in preprocessing  all_real_data shape\\n',all_real_data.shape)\n",
    "    #对于所有的缺失值均产生于CATEGORICAL_COLUMNS中，所以这里考虑使用0填充 \n",
    "    #all_real_data.fillna(0,inplace=True)\n",
    "    all_data = all_real_data.copy(deep=True)\n",
    "    # 标签2值化\n",
    "    all_data[LABEL_COLUMN] = all_data['评分'].map(lambda x: 1 if x>=6 else 0 )\n",
    "    #all_data.pop('评分')\n",
    "    # 标签y\n",
    "    y = all_data[LABEL_COLUMN].values\n",
    "    # 数据X\n",
    "    all_data.pop(LABEL_COLUMN)\n",
    "    train_size = int(all_data.shape[0])#*0.95\n",
    "    x_train = all_data.iloc[:train_size]\n",
    "    list_all_columns = list(x_train.columns)\n",
    "    y_train = y[:train_size]\n",
    "    # 类别型的列\n",
    "    dftest = x_train[CATEGORICAL_COLUMNS]\n",
    "    print('dftest shape\\n',dftest.shape)\n",
    "    x_train_categ = np.array(x_train[CATEGORICAL_COLUMNS])\n",
    "    # 连续值的列\n",
    "    x_train_conti = np.array(x_train[CONTINUOUS_COLUMNS], dtype='float64')\n",
    "    # 对连续值的列做幅度缩放\n",
    "    scaler = StandardScaler()\n",
    "    x_train_conti = scaler.fit_transform(x_train_conti)\n",
    "    del x_train['用户名']\n",
    "    del x_train['评论时间']\n",
    "    del x_train['主演']\n",
    "    del x_train['导演']\n",
    "    del x_train['电影名']\n",
    "    del x_train['用户ID']\n",
    "    return [x_train, y_train,   x_train_categ,  x_train_conti,  all_data]\n",
    "\n",
    "class Wide_and_Deep:\n",
    "    def __init__(self, data,label_column,categorie_column,continus_column,mode='wide and deep'):\n",
    "        self.data = data\n",
    "        self.mode = mode\n",
    "        self.label_column = label_column\n",
    "        self.categorie_column = categorie_column\n",
    "        self.continus_column = continus_column\n",
    "        \n",
    "        x_train, y_train,  x_train_categ,  x_train_conti,  all_data \\\n",
    "            = preprocessing(self.data,label_column,categorie_column,continus_column)\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        print('self.x_train step2\\n',self.x_train.shape)\n",
    "        #for i in list(x_train.columns):\n",
    "        #    print(i)\n",
    "        print('self.y_train step2\\n',self.y_train.shape)\n",
    "        self.x_train_categ = x_train_categ\n",
    "        self.x_train_conti = x_train_conti\n",
    "        self.all_data = all_data\n",
    "        #由于特征列太多。交叉后会产生(239445, 52003) #degree=1 不交叉\n",
    "        self.poly = PolynomialFeatures(degree=1, interaction_only=True,include_bias=False)\n",
    "        self.x_train_categ_poly = self.poly.fit_transform(x_train_categ)\n",
    "        self.categ_inputs = None\n",
    "        self.conti_input = None\n",
    "        self.deep_component_outlayer = None\n",
    "        self.logistic_input = None\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "    def deep_component(self):\n",
    "        # deep部分的组件\n",
    "        categ_inputs = []\n",
    "        categ_embeds = []\n",
    "        # 对类别型的列做embedding\n",
    "        count = 0\n",
    "        list_all = list(self.all_data.columns)\n",
    "\n",
    "        for i in range(len(self.categorie_column)):\n",
    "            input_i = Input(shape=(1,), dtype='int32')\n",
    "            dim = len(np.unique(self.all_data[self.categorie_column[i]]))\n",
    "            #print('dim value is\\n',dim)\n",
    "            #print('不同元素个数',self.all_data[self.categorie_column[i]].nunique())\n",
    "            embed_dim = int(np.ceil(dim ** 0.25))\n",
    "            embed_i = Embedding(200, embed_dim, input_length=1)(input_i)\n",
    "            flatten_i = Flatten()(embed_i)\n",
    "            categ_inputs.append(input_i)\n",
    "            categ_embeds.append(flatten_i)\n",
    "        # 连续值的列\n",
    "        conti_input = Input(shape=(len(self.continus_column),))\n",
    "        conti_dense = Dense(256, use_bias=False)(conti_input)\n",
    "        # 拼接类别型的embedding和连续值特征\n",
    "        concat_embeds = concatenate([conti_dense] + categ_embeds)\n",
    "        # 激活层与BN层\n",
    "        concat_embeds = Activation('relu')(concat_embeds)\n",
    "        bn_concat = BatchNormalization()(concat_embeds)\n",
    "        # 全连接+激活层+BN层\n",
    "        fc1 = Dense(512, use_bias=False)(bn_concat)\n",
    "        ac1 = ReLU()(fc1)\n",
    "        bn1 = BatchNormalization()(ac1)\n",
    "        fc2 = Dense(256, use_bias=False)(bn1)\n",
    "        ac2 = ReLU()(fc2)\n",
    "        bn2 = BatchNormalization()(ac2)\n",
    "        fc3 = Dense(128)(bn2)\n",
    "        ac3 = ReLU()(fc3)\n",
    "\n",
    "        self.categ_inputs = categ_inputs\n",
    "        self.conti_input = conti_input\n",
    "        self.deep_component_outlayer = ac3\n",
    "\n",
    "    # self.x_train_categ_poly.shape[1] --> 37\n",
    "    def wide_component(self):\n",
    "        # wide部分的组件\n",
    "        dim = self.x_train_categ_poly.shape[1]\n",
    "        self.logistic_input = Input(shape=(dim,))\n",
    "\n",
    "    # X           *   W      = Y\n",
    "    # (None, 165) * (165,1)  = (None,1)\n",
    "    def create_model(self):\n",
    "        # wide+deep\n",
    "        self.deep_component()\n",
    "        self.wide_component()\n",
    "        if self.mode == 'wide and deep':\n",
    "            out_layer = concatenate([self.deep_component_outlayer, self.logistic_input])\n",
    "            inputs = [self.conti_input] + self.categ_inputs + [self.logistic_input]\n",
    "        elif self.mode == 'deep':\n",
    "            out_layer = self.deep_component_outlayer\n",
    "            inputs = [self.conti_input] + self.categ_inputs\n",
    "        else:\n",
    "            print('wrong mode')\n",
    "            return\n",
    "\n",
    "        output = Dense(1, activation='sigmoid')(out_layer)\n",
    "        self.model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # 训练\n",
    "    # x: 训练数据的 Numpy 数组\n",
    "    # y: 目标（标签）数据的 Numpy 数组\n",
    "    # self.model.fit(x=None, y=None,epochs=epochs, batch_size=batch_size)\n",
    "    def train_model(self, epochs=2, optimizer='adam', batch_size=128):\n",
    "        # 不同结构的训练\n",
    "\n",
    "        # 没有model的情况\n",
    "        if not self.model:\n",
    "            print('You have to create model first')\n",
    "            return\n",
    "\n",
    "        # 使用wide&deep的情况\n",
    "        if self.mode == 'wide and deep':\n",
    "            #self.x_train_categ_poly shape\n",
    "            # (47673, 310)\n",
    "            print('------------------------------------------\\n')\n",
    "            print('self.x_train_conti shape\\n',self.x_train_conti.shape)\n",
    "            print(' self.x_train_categ shape[1] \\n',self.x_train_categ.shape[1])\n",
    "            print('self.x_train_categ_poly shape\\n',self.x_train_categ_poly.shape)\n",
    "            print('------------------------------------------\\n')\n",
    "            input_data = [self.x_train_conti] + \\\n",
    "                         [self.x_train_categ[:, i] for i in range(self.x_train_categ.shape[1])] + \\\n",
    "                         [self.x_train_categ_poly]\n",
    "            print(' wide and deep is \\n', len(input_data) )\n",
    "        # 只使用deep的情况\n",
    "        elif self.mode == 'deep':\n",
    "            input_data = [self.x_train_conti] + \\\n",
    "                         [self.x_train_categ[:, i] for i in range(self.x_train_categ.shape[1])]\n",
    "        else:\n",
    "            print('wrong mode')\n",
    "            return\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', ])\n",
    "        print('  self.y_train is \\n', self.y_train.shape)\n",
    "        \n",
    "        self.model.fit(input_data, self.y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # 评估\n",
    "    def evaluate_model(self):\n",
    "        # if not self.model:\n",
    "        #     print('You have to create model first')\n",
    "        #     return\n",
    "        #\n",
    "        # if self.mode == 'wide and deep':\n",
    "        #     input_data = [self.x_test_conti] + \\\n",
    "        #                  [self.x_test_categ[:, i] for i in range(self.x_test_categ.shape[1])] + \\\n",
    "        #                  [self.x_test_categ_poly]\n",
    "        #     print('input_data len', len(input_data))\n",
    "        # elif self.mode == 'deep':\n",
    "        #     input_data = [self.x_test_conti] + \\\n",
    "        #                  [self.x_test_categ[:, i] for i in range(self.x_test_categ.shape[1])]\n",
    "        # else:\n",
    "        #     print('wrong mode')\n",
    "        #     return\n",
    "        #\n",
    "        # loss, acc = self.model.evaluate(input_data, self.y_test)\n",
    "        # print(f'test_loss: {loss} - test_acc: {acc}')\n",
    "        pass\n",
    "\n",
    "    def save_model(self, filename='./wide_and_deep_9train.h5'):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    # 预测\n",
    "    def predict_model(self):\n",
    "#         if not self.model:\n",
    "#             print('You have to create model first')\n",
    "#             return\n",
    "\n",
    "#         if self.mode == 'wide and deep':\n",
    "#             input_data = [self.x_all_conti] + \\\n",
    "#                          [self.x_all_categ[:, i] for i in range(self.x_all_categ.shape[1])] + \\\n",
    "#                          [self.x_all_categ_poly]\n",
    "#         elif self.mode == 'deep':\n",
    "#             input_data = [self.x_all_conti] + \\\n",
    "#                          [self.x_all_categ[:, i] for i in range(self.x_all_categ.shape[1])]\n",
    "#         else:\n",
    "#             print('wrong mode')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#离线训练，预测\n",
    "def offline_test():\n",
    "    train_start_date = '2017-12-31'\n",
    "    train_end_date = '2018-01-30'\n",
    "    test_start_date = '2018-01-10' \n",
    "    test_end_date = '2018-02-07'\n",
    "    train_X, train_Y = make_train_set(train_start_date, train_end_date,span=30)\n",
    "    assert train_X.shape[0] == train_Y.shape[0]\n",
    "    COLUMNS = list(train_X.columns)\n",
    "    print('train_X columns len\\n',len(COLUMNS))\n",
    "    LABEL_COLUMN = \"喜欢\"\n",
    "    CATEGORICAL_COLUMNS = list(filter(lambda x : '_' in x,COLUMNS))\n",
    "    print('CATEGORICAL_COLUMNS len\\n',len(CATEGORICAL_COLUMNS))\n",
    "    #print('CATEGORICAL_COLUMNS\\n',CATEGORICAL_COLUMNS)\n",
    "    #评分 用户名 评论时间 用户ID 电影名 主演 导演 豆瓣网评分\n",
    "    CONTINUOUS_COLUMNS = [\"评分\", \"豆瓣网评分\"]\n",
    "    #print('CONTINUOUS_COLUMNS\\n',CONTINUOUS_COLUMNS)\n",
    "    print('train_X shape\\n',train_X.shape)\n",
    "    wide_deep_net = Wide_and_Deep(train_X,LABEL_COLUMN,CATEGORICAL_COLUMNS,CONTINUOUS_COLUMNS)\n",
    "    wide_deep_net.create_model()\n",
    "    wide_deep_net.train_model()\n",
    "    wide_deep_net.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set   shape\n",
      " (239445, 8)\n",
      "label shape \n",
      " (171165, 3)\n",
      "start_date.hour is\n",
      " 16\n",
      "train_X columns len\n",
      " 318\n",
      "CATEGORICAL_COLUMNS len\n",
      " 310\n",
      "train_X shape\n",
      " (47673, 318)\n",
      "-----------in preprocessing -----\n",
      "\n",
      "-----------in preprocessing  all_real_data shape\n",
      " (47673, 318)\n",
      "dftest shape\n",
      " (47673, 310)\n",
      "self.x_train step2\n",
      " (47673, 312)\n",
      "self.y_train step2\n",
      " (47673,)\n",
      "------------------------------------------\n",
      "\n",
      "self.x_train_conti shape\n",
      " (47673, 2)\n",
      " self.x_train_categ shape[1] \n",
      " 310\n",
      "self.x_train_categ_poly shape\n",
      " (47673, 310)\n",
      "------------------------------------------\n",
      "\n",
      " wide and deep is \n",
      " 312\n",
      "  self.y_train is \n",
      " (47673,)\n",
      "Train on 47673 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000028D1303FD08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000028D1303FD08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "47673/47673 [==============================] - 48s 998us/sample - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 2/2\n",
      "47673/47673 [==============================] - 18s 381us/sample - loss: 4.9948e-05 - accuracy: 1.0000- loss: 5.2\n"
     ]
    }
   ],
   "source": [
    "offline_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set = pd.read_csv('./train_set_stepone.csv')\n",
    "df_train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set.用户ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen_user_action_feat_day = pd.read_csv('./gen_user_action_feat_day.csv')\n",
    "df_gen_user_action_feat_day.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4758"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen_user_action_feat_day.用户ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>评分</th>\n",
       "      <th>用户名</th>\n",
       "      <th>评论时间</th>\n",
       "      <th>用户ID</th>\n",
       "      <th>电影名</th>\n",
       "      <th>主演</th>\n",
       "      <th>导演</th>\n",
       "      <th>豆瓣网评分</th>\n",
       "      <th>1_day_user_type_0</th>\n",
       "      <th>1_day_user_type_1</th>\n",
       "      <th>...</th>\n",
       "      <th>1_day_user_area_13_avg</th>\n",
       "      <th>1_day_user_area_14_avg</th>\n",
       "      <th>1_day_user_area_15_avg</th>\n",
       "      <th>1_day_user_area_16_avg</th>\n",
       "      <th>1_day_user_area_17_avg</th>\n",
       "      <th>1_day_user_area_18_avg</th>\n",
       "      <th>1_day_user_area_19_avg</th>\n",
       "      <th>1_day_user_area_20_avg</th>\n",
       "      <th>1_day_user_area_sum_y.1</th>\n",
       "      <th>1_day_user_active_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>身似</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>1</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>身似</td>\n",
       "      <td>2018-01-05 15:05:06</td>\n",
       "      <td>1</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>缘三</td>\n",
       "      <td>2018-01-08 12:24:36</td>\n",
       "      <td>7884</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>缘三</td>\n",
       "      <td>2018-01-08 12:24:36</td>\n",
       "      <td>7884</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Moss大妖</td>\n",
       "      <td>2018-01-18 13:30:05</td>\n",
       "      <td>4919</td>\n",
       "      <td>心雨花露</td>\n",
       "      <td>姬晨牧</td>\n",
       "      <td>喻瀚湫</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   评分     用户名                 评论时间  用户ID   电影名   主演   导演  豆瓣网评分  \\\n",
       "0   2      身似  2018-01-05 15:05:06     1  心雨花露  姬晨牧  喻瀚湫    6.4   \n",
       "1   2      身似  2018-01-05 15:05:06     1  心雨花露  姬晨牧  喻瀚湫    6.4   \n",
       "2  10      缘三  2018-01-08 12:24:36  7884  心雨花露  姬晨牧  喻瀚湫    6.4   \n",
       "3  10      缘三  2018-01-08 12:24:36  7884  心雨花露  姬晨牧  喻瀚湫    6.4   \n",
       "4   8  Moss大妖  2018-01-18 13:30:05  4919  心雨花露  姬晨牧  喻瀚湫    6.4   \n",
       "\n",
       "   1_day_user_type_0  1_day_user_type_1  ...  1_day_user_area_13_avg  \\\n",
       "0                NaN                NaN  ...                     NaN   \n",
       "1                NaN                NaN  ...                     NaN   \n",
       "2                0.0                0.0  ...                     0.0   \n",
       "3                0.0                0.0  ...                     0.0   \n",
       "4                0.0                0.0  ...                     0.0   \n",
       "\n",
       "   1_day_user_area_14_avg  1_day_user_area_15_avg  1_day_user_area_16_avg  \\\n",
       "0                     NaN                     NaN                     NaN   \n",
       "1                     NaN                     NaN                     NaN   \n",
       "2                     0.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     0.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   1_day_user_area_17_avg  1_day_user_area_18_avg  1_day_user_area_19_avg  \\\n",
       "0                     NaN                     NaN                     NaN   \n",
       "1                     NaN                     NaN                     NaN   \n",
       "2                     0.0                     0.0                     0.0   \n",
       "3                     0.0                     0.0                     0.0   \n",
       "4                     0.0                     0.0                     0.0   \n",
       "\n",
       "   1_day_user_area_20_avg  1_day_user_area_sum_y.1  1_day_user_active_days  \n",
       "0                     NaN                      NaN                     NaN  \n",
       "1                     NaN                      NaN                     NaN  \n",
       "2                     0.0                      2.0                     1.0  \n",
       "3                     0.0                      2.0                     1.0  \n",
       "4                     0.0                      1.0                     1.0  \n",
       "\n",
       "[5 rows x 268 columns]"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 =   pd.merge(df_train_set, df_gen_user_action_feat_day, how='left', on='用户ID')\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13537"
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.用户ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        评分     用户名                 评论时间   用户ID    电影名  \\\n",
      "0        2      身似  2018-01-05 15:05:06      1   心雨花露   \n",
      "1        2      身似  2018-01-05 15:05:06      1   心雨花露   \n",
      "6        4      陆由  2018-01-19 16:11:20  21484   心雨花露   \n",
      "7        4      陆由  2018-01-19 16:11:20  21484   心雨花露   \n",
      "8        8  ewquyy  2018-01-19 16:27:27   8791   心雨花露   \n",
      "...     ..     ...                  ...    ...    ...   \n",
      "239416   8    绿色阳光  2018-01-28 13:41:49   7255  足球流氓2   \n",
      "239420  10   小僵尸蹲蹲  2018-01-28 14:23:20  52918   一无所有   \n",
      "239422   4      玑衡  2018-01-28 14:23:58   1490  电视台风云   \n",
      "239423   2    麻木斯基  2018-01-28 14:35:01   3534   黑夜传说   \n",
      "239426   8      卢比  2018-01-28 15:16:55  10155   花桥荣记   \n",
      "\n",
      "                                        主演       导演  豆瓣网评分  1_day_user_type_0  \\\n",
      "0                                      姬晨牧      喻瀚湫    6.4                NaN   \n",
      "1                                      姬晨牧      喻瀚湫    6.4                NaN   \n",
      "6                                      姬晨牧      喻瀚湫    6.4                NaN   \n",
      "7                                      姬晨牧      喻瀚湫    6.4                NaN   \n",
      "8                                      姬晨牧      喻瀚湫    6.4                NaN   \n",
      "...                                    ...      ...    ...                ...   \n",
      "239416                     罗斯·麦克科尔|特雷瓦·埃迪内   杰西·约翰逊    5.9                NaN   \n",
      "239420        蒂莫西·斯波|莱丝利·曼维尔|詹姆斯·柯登|莎莉·霍金斯     迈克·李    8.1                NaN   \n",
      "239422    费·唐纳薇|威廉·霍尔登|彼得·芬奇|罗伯特·杜瓦尔|尼德·巴蒂  西德尼·吕美特    8.4                NaN   \n",
      "239423  凯特·贝金赛尔|斯科特·斯比德曼|山恩·布罗利|麦克·辛|比尔·奈伊    伦·怀斯曼    7.4                NaN   \n",
      "239426                           郑裕玲|郁芳|周迅       谢衍    7.6                NaN   \n",
      "\n",
      "        1_day_user_type_1  ...  1_day_user_area_13_avg  \\\n",
      "0                     NaN  ...                     NaN   \n",
      "1                     NaN  ...                     NaN   \n",
      "6                     NaN  ...                     NaN   \n",
      "7                     NaN  ...                     NaN   \n",
      "8                     NaN  ...                     NaN   \n",
      "...                   ...  ...                     ...   \n",
      "239416                NaN  ...                     NaN   \n",
      "239420                NaN  ...                     NaN   \n",
      "239422                NaN  ...                     NaN   \n",
      "239423                NaN  ...                     NaN   \n",
      "239426                NaN  ...                     NaN   \n",
      "\n",
      "        1_day_user_area_14_avg  1_day_user_area_15_avg  \\\n",
      "0                          NaN                     NaN   \n",
      "1                          NaN                     NaN   \n",
      "6                          NaN                     NaN   \n",
      "7                          NaN                     NaN   \n",
      "8                          NaN                     NaN   \n",
      "...                        ...                     ...   \n",
      "239416                     NaN                     NaN   \n",
      "239420                     NaN                     NaN   \n",
      "239422                     NaN                     NaN   \n",
      "239423                     NaN                     NaN   \n",
      "239426                     NaN                     NaN   \n",
      "\n",
      "        1_day_user_area_16_avg  1_day_user_area_17_avg  \\\n",
      "0                          NaN                     NaN   \n",
      "1                          NaN                     NaN   \n",
      "6                          NaN                     NaN   \n",
      "7                          NaN                     NaN   \n",
      "8                          NaN                     NaN   \n",
      "...                        ...                     ...   \n",
      "239416                     NaN                     NaN   \n",
      "239420                     NaN                     NaN   \n",
      "239422                     NaN                     NaN   \n",
      "239423                     NaN                     NaN   \n",
      "239426                     NaN                     NaN   \n",
      "\n",
      "        1_day_user_area_18_avg  1_day_user_area_19_avg  \\\n",
      "0                          NaN                     NaN   \n",
      "1                          NaN                     NaN   \n",
      "6                          NaN                     NaN   \n",
      "7                          NaN                     NaN   \n",
      "8                          NaN                     NaN   \n",
      "...                        ...                     ...   \n",
      "239416                     NaN                     NaN   \n",
      "239420                     NaN                     NaN   \n",
      "239422                     NaN                     NaN   \n",
      "239423                     NaN                     NaN   \n",
      "239426                     NaN                     NaN   \n",
      "\n",
      "        1_day_user_area_20_avg  1_day_user_area_sum_y.1  \\\n",
      "0                          NaN                      NaN   \n",
      "1                          NaN                      NaN   \n",
      "6                          NaN                      NaN   \n",
      "7                          NaN                      NaN   \n",
      "8                          NaN                      NaN   \n",
      "...                        ...                      ...   \n",
      "239416                     NaN                      NaN   \n",
      "239420                     NaN                      NaN   \n",
      "239422                     NaN                      NaN   \n",
      "239423                     NaN                      NaN   \n",
      "239426                     NaN                      NaN   \n",
      "\n",
      "        1_day_user_active_days  \n",
      "0                          NaN  \n",
      "1                          NaN  \n",
      "6                          NaN  \n",
      "7                          NaN  \n",
      "8                          NaN  \n",
      "...                        ...  \n",
      "239416                     NaN  \n",
      "239420                     NaN  \n",
      "239422                     NaN  \n",
      "239423                     NaN  \n",
      "239426                     NaN  \n",
      "\n",
      "[120909 rows x 268 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1[df1.isnull().T.any()])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
